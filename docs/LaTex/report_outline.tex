% ============================================================
%  IoT-Based Intelligent Harvest Sorting & Quality Analysis
%  Academic Research Paper – Outline
% ============================================================

\documentclass[12pt, a4paper, oneside]{report}

% ── Packages ────────────────────────────────────────────────
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
  \geometry{margin=2.5cm}
\usepackage{setspace}
  \onehalfspacing
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amsmath, amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{hyperref}
  \hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}
\usepackage[backend=biber, style=ieee]{biblatex}
  \addbibresource{references.bib}
\usepackage{acronym}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
  \usetikzlibrary{shapes, arrows.meta, positioning}

% ── Meta ────────────────────────────────────────────────────
\title{
  \textbf{IoT-Based Intelligent Harvest Sorting\\
  and Quality Analysis System}\\[6pt]
  \large A Microservices Architecture Integrating Classical Computer Vision,\\
  Deep Learning, and Generative AI for Automated Crop Quality Control
}
\author{
  \textbf{Author Name}\\
  Department of Software Engineering\\
  University Name\\
  \texttt{author@university.edu}
}
\date{Academic Year 2025 -- 2026}

% ============================================================
\begin{document}

\pagestyle{plain}

\maketitle
\newpage

% ── Abstract ────────────────────────────────────────────────
\begin{abstract}
  % ≈ 250 words
  % 1. Context & motivation
  % 2. Problem statement
  % 3. Proposed approach (two-stage pipeline, microservices architecture)
  % 4. Key results (accuracy, latency, throughput)
  % 5. Conclusion & significance
  \textit{(To be written after experimentation.)}
\end{abstract}

\tableofcontents
\listoffigures
\listoftables

% % ── Acronyms ────────────────────────────────────────────────
\chapter*{List of Acronyms}
\addcontentsline{toc}{chapter}{List of Acronyms}
% \begin{acronym}[IIoT]
%   \acro{CNN}{Convolutional Neural Network}
%   \acro{API}{Application Programming Interface}
%   \acro{IoT}{Internet of Things}
%   \acro{IIoT}{Industrial Internet of Things}
%   \acro{HSV}{Hue, Saturation, Value}
%   \acro{LLM}{Large Language Model}
%   \acro{REST}{Representational State Transfer}
%   \acro{GAP}{Global Average Pooling}
% \end{acronym}
\begin{table}[h!]
    \centering
    \large
    \label{tab:acronyms}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Acronym} & \textbf{Full Term} \\ \hline
        CNN & Convolutional Neural Network \\ \hline
        API & Application Programming Interface \\ \hline
        IoT & Internet of Things \\ \hline
        IIoT & Industrial Internet of Things \\ \hline
        HSV & Hue, Saturation, Value \\ \hline
        LLM & Large Language Model \\ \hline
        REST & Representational State Transfer \\ \hline
        GAP & Global Average Pooling \\ \hline
    \end{tabular}
\end{table}
















% ============================================================
%                   CHAPTER 1 – INTRODUCTION
% ============================================================
\chapter{General Introduction}
\label{ch:introduction}

% ────────────────────────────────────────────────────────────
\section{Context and Motivation}
\label{sec:context}
% ────────────────────────────────────────────────────────────

The date palm (\textit{Phoenix dactylifera} L.) is one of the oldest cultivated
fruit trees, with significant economic and cultural importance across the
Middle East and North Africa.

According to the Food and Agriculture Organization of the United Nations
(FAO), global date production reached approximately
\textbf{10.09 million tonnes} in \textbf{2024}, with
\textbf{Saudi Arabia}, \textbf{Egypt}, and \textbf{Algeria}
being the leading producers~\cite{fao_dates_production}.

Despite their economic value, dates are highly susceptible to quality
degradation caused by fungal infection, insect infestation, excessive
moisture, and mechanical damage during harvesting and
transportation~\cite{almomen2023date}.
Post-harvest losses in the date sector are estimated at
\textbf{30\%} of the total annual yield in several producing
regions~\cite{fao_food_loss}.

In traditional packing facilities, quality grading is still predominantly
performed by human inspectors who visually assess each fruit for colour,
texture, size, and the presence of defects.  This manual process is
inherently slow, subjective, and non-reproducible: two different operators
may assign different grades to the same fruit, and fatigue significantly
degrades accuracy over long shifts~\cite{almomen2023date}.

The convergence of the Internet of Things (IoT), computer vision, and deep
learning offers a promising path toward automating this process.  Modern
lightweight Convolutional Neural Networks (CNNs) such as MobileNetV2 can
classify images with high accuracy while remaining deployable on
resource-constrained environments~\cite{sandler2018mobilenetv2}.
When combined with classical image processing for object isolation and
cloud-based inference services, a complete, non-destructive, and scalable
quality control pipeline becomes feasible.

This project is therefore motivated by the need for an
\textbf{end-to-end intelligent system} that can:
\begin{enumerate}
    \item detect and isolate individual date fruits from a conveyor-belt
          camera feed,
    \item classify each fruit's quality in real time, and
    \item log, visualise, and \emph{interpret} the production data
          through generative AI.
\end{enumerate}

% ────────────────────────────────────────────────────────────
\section{Problem Statement}
\label{sec:problem}
% ────────────────────────────────────────────────────────────

Although several studies have explored CNN-based classification of date
fruits~\cite{almomen2023date, altaheri2019date}, most focus exclusively on
the classification task in isolation.  The broader engineering challenge of
integrating detection, classification, data persistence, and operational
decision support into a single deployable system remains largely
unaddressed.  The following specific gaps motivate the present work:

\begin{enumerate}
    \item \textbf{Subjectivity of manual grading.}
    Human inspectors remain the primary quality gate in most date packing
    facilities.  Their assessments are influenced by lighting, fatigue, and
    individual perception, leading to inconsistent grading across shifts and
    facilities.

    \item \textbf{Computational cost of detection-only deep learning
    pipelines.}
    End-to-end object-detection models such as YOLO~\cite{redmon2016yolo}
    achieve high accuracy but require significant GPU resources.  For
    scenarios where the background is controlled (e.g., a white conveyor
    belt), classical computer vision techniques can isolate objects at a
    fraction of the cost.

    \item \textbf{Absence of integrated quality control workflows.}
    Existing research typically stops at reporting a classification accuracy
    score.  A production-grade system must also handle image ingestion from
    an IoT device, persist predictions in a database, and surface
    actionable insights — not just raw numbers — to operations managers.
\end{enumerate}

The central question addressed in this work can be formulated as:

\begin{quote}
\textit{How can a lightweight, cloud-deployable system be designed to
automatically detect, classify, and log the quality of date fruits in real
time, while providing AI-generated operational insights to production
managers?}
\end{quote}

% ────────────────────────────────────────────────────────────
\section{Objectives}
\label{sec:objectives}
% ────────────────────────────────────────────────────────────

To answer the research question stated above, four operational objectives
are defined:

\begin{enumerate}
    \item[\textbf{O1.}] \textbf{Design a two-stage vision pipeline} that
    combines classical computer vision (colour-space transformation,
    Otsu's thresholding, morphological operations) for object isolation
    with a pre-trained MobileNetV2 CNN for quality classification.

    \item[\textbf{O2.}] \textbf{Implement a containerised cloud API}
    using FastAPI and Docker, capable of receiving images over HTTP,
    performing inference entirely in memory, and returning predictions
    with sub-second latency.

    \item[\textbf{O3.}] \textbf{Simulate an IoT edge device} that mimics
    a conveyor-mounted industrial camera, continuously capturing and
    transmitting images to the cloud service with built-in network
    resilience.

    \item[\textbf{O4.}] \textbf{Persist inspection data and generate
    AI-driven quality reports} by logging every prediction to a
    PostgreSQL database (Supabase) and integrating a Large Language
    Model (Google Gemini) to interpret production trends and generate
    managerial recommendations.
\end{enumerate}

% ────────────────────────────────────────────────────────────
\section{Proposed Approach}
\label{sec:approach}
% ────────────────────────────────────────────────────────────

The proposed system follows a four-layer microservices architecture,
illustrated in Figure~\ref{fig:high_level_arch}.

\begin{figure}[H]
    \centering
    \caption{High-level architecture of the proposed system.}
    \includegraphics[width=0.88\textwidth]{figures/ch1_architecture.png}
    \label{fig:high_level_arch}
\end{figure}

\begin{itemize}
    \item \textbf{Edge Layer.} A Python script (\texttt{iot\_simulation.py})
    simulates a camera mounted above a conveyor belt.  It iterates over a
    folder of test images, transmitting each one to the cloud API via HTTP
    POST with a configurable delay to mimic the belt speed.

    \item \textbf{Application Layer.} A FastAPI application
    (\texttt{api.py}), containerised with Docker and hosted on Render,
    receives the uploaded image, decodes it in memory (zero-disk
    architecture), passes it through the two-stage pipeline
    (detection → classification), and returns a JSON response containing
    the predicted class and confidence score.

    \item \textbf{Data Persistence Layer.}  Every prediction is
    asynchronously logged to a Supabase PostgreSQL database, creating a
    historical record suitable for time-series analysis without blocking
    the inference response.

    \item \textbf{Management Layer.} A Streamlit dashboard queries the
    database, visualises production metrics (rejection rate, class
    distribution over time), and can invoke Google Gemini to generate
    natural-language quality reports acting as a virtual Quality Manager.
\end{itemize}

The two-stage vision pipeline — which is the core technical contribution —
first applies classical computer vision (HSV conversion, Gaussian blur,
Otsu's adaptive thresholding, and morphological closing/opening) to
isolate individual date fruits from the background, and then feeds each
cropped region into a MobileNetV2 CNN pre-trained on ImageNet and
fine-tuned on date fruit images for binary classification
(\textit{Fresh}~vs.~\textit{Dry}).

% ────────────────────────────────────────────────────────────
\section{Scope and Limitations}
\label{sec:scope}
% ────────────────────────────────────────────────────────────

The boundaries of this work were defined to ensure feasibility within a
single-semester Master's module:

\paragraph{In scope.}
\begin{itemize}
    \item Binary classification: \textit{Fresh} (Grade~1) versus
          \textit{Dry/Rotten} (Grade~3).
    \item A simulated IoT camera client (software-based, no physical
          hardware).
    \item A single commodity: date fruits, using the
          \textit{Augmented Date Fruit Dataset}.
    \item Cloud deployment on a free-tier platform (Render).
    \item LLM-assisted report generation via the Google Gemini API.
\end{itemize}

\paragraph{Out of scope.}
\begin{itemize}
    \item Multi-class grading (e.g., Grade~1 / Grade~2 / Grade~3).
    \item Physical hardware integration (Raspberry Pi, industrial camera,
          or robotic rejection arm).
    \item On-device edge inference (e.g., TensorFlow Lite quantisation).
    \item Fine-tuning or hosting a local LLM.
\end{itemize}

\paragraph{Acknowledged limitations.}
\begin{itemize}
    \item The augmented dataset may not fully represent the variability
          encountered in real-world packing lines (dust, partial
          occlusion, conveyor vibration).
    \item Free-tier cloud hosting introduces cold-start latency of
          up to 30--60 seconds after periods of inactivity.
    \item LLM-generated reports are subject to hallucination and must be
          reviewed by a human operator before acting on recommendations.
\end{itemize}

% ────────────────────────────────────────────────────────────
\section{Document Structure}
\label{sec:structure}
% ────────────────────────────────────────────────────────────

The remainder of this report is organised as follows:

\begin{description}
    \item[Chapter~\ref{ch:literature}] reviews the state of the art in
    post-harvest quality inspection, classical computer vision,
    CNN-based classification, IoT architectures for agriculture, and
    the use of large language models for operational decision support.

    \item[Chapter~\ref{ch:design}] presents the system design, including
    the microservices architecture, cloud API structure, database schema,
    and dashboard design.

    \item[Chapter~\ref{ch:preprocessing}] details the classical computer
    vision preprocessing pipeline used for object detection and isolation.

    \item[Chapter~\ref{ch:training}] describes dataset preparation,
    the MobileNetV2 transfer-learning strategy, and the training
    configuration.

    \item[Chapter~\ref{ch:results}] reports experimental results,
    including classification metrics, system latency benchmarks, and a
    qualitative assessment of LLM-generated reports.

    \item[Chapter~\ref{ch:discussion}] provides a critical discussion of
    the results, strengths, and limitations.

    \item[Chapter~\ref{ch:conclusion}] concludes with a summary of
    contributions and directions for future work.
\end{description}

























% ============================================================
%                 CHAPTER 2 – LITERATURE REVIEW
% ============================================================
\chapter{Literature Review and State of the Art}
\label{ch:literature}

This chapter reviews the key disciplines that underpin the proposed system:
post-harvest inspection practices, classical computer vision,
CNN-based image classification, IoT architectures, and the emerging use
of generative AI for operational decision support.  Each section concludes
with a justification of the technology selected for this project.
A comparative table of related work and a synthesis of identified gaps
close the chapter.

% ────────────────────────────────────────────────────────────
\section{Post-Harvest Quality Inspection}
\label{sec:postharvest}
% ────────────────────────────────────────────────────────────

Date fruit quality is traditionally assessed by human graders who
evaluate colour, texture, size, and the presence of surface
defects~\cite{almomen2023date}.  While effective for small batches,
manual inspection introduces three well-documented problems:
(i)~\textit{subjectivity} — different operators may assign different
grades to the same fruit;
(ii)~\textit{fatigue} — accuracy drops significantly during extended
shifts; and
(iii)~\textit{lack of traceability} — no digital record is created
for each decision.

Instrumental methods such as colourimeters and refractometers offer
objective measurements but are destructive and unsuitable for
high-throughput conveyor lines.  This has driven interest in
\textit{non-destructive, vision-based} inspection systems that can
capture and classify hundreds of fruits per
minute~\cite{szeliski2022computer}.

\paragraph{Justification.}
The limitations above motivate our choice of an
\textit{automated, camera-based pipeline} that produces a persistent
digital log for every inspected fruit.

% ────────────────────────────────────────────────────────��───
\clearpage
\section{Classical Computer Vision for Object Segmentation}
\label{sec:classical_cv}
% ────────────────────────────────────────────────────────────

Before a classifier can operate, individual objects must be isolated
from the background.  Classical computer vision provides lightweight
algorithms well-suited to \textit{controlled environments} such as
industrial conveyor belts with uniform backgrounds.

\subsection{Colour-Space Transformations}

The RGB colour model is sensitive to illumination changes.  Converting
an image to the \textbf{HSV} (Hue, Saturation, Value) space decouples
chromatic content from brightness, making it easier to distinguish
coloured objects from grey shadows~\cite{szeliski2022computer}.
Table~\ref{tab:colour_spaces} summarises the three most common colour
spaces used in agricultural vision.

\begin{table}[H]
\centering
\caption{Comparison of colour spaces for fruit segmentation.}
\label{tab:colour_spaces}
\begin{tabularx}{\textwidth}{l X X}
\toprule
\textbf{Space} & \textbf{Advantage} & \textbf{Limitation} \\
\midrule
RGB   & Native camera format, no conversion needed
      & Highly sensitive to lighting changes \\
HSV   & Separates colour (H) from intensity (V); robust shadow rejection
      & Hue wraps around at 0°/360° \\
L*a*b* & Perceptually uniform; good for colour-distance metrics
       & Higher computational cost \\
\bottomrule
\end{tabularx}
\end{table}

\paragraph{Justification.}
Our system converts images to HSV and operates on the
\textbf{Saturation channel} specifically.  Date fruits — whether fresh
or dry — are chromatically rich (high S), whereas conveyor-belt shadows
are achromatic (low S).  This single-channel approach eliminates shadows
without the added cost of L*a*b* conversion.

\subsection{Thresholding Techniques}

Thresholding converts a grey-scale image into a binary mask.  Fixed
(global) thresholds fail when lighting conditions vary.
\textbf{Otsu's method}~\cite{otsu1979threshold} computes the optimal
threshold automatically by maximising the inter-class variance between
foreground and background pixels, making it adaptive to each frame.

\subsection{Morphological Operations}

Binary masks produced by thresholding often contain holes (caused by
fruit textures) and noise (small white specks).
\textit{Morphological closing} — a dilation followed by an erosion —
fills internal gaps, while \textit{morphological opening} — an erosion
followed by a dilation — removes small noise.  Together, they produce
a clean, solid foreground mask suitable for contour
detection~\cite{szeliski2022computer}.

\begin{figure}[H]
    \centering
    \caption{Visualisation of the foreground mask creation pipeline:
      Saturation channel, Otsu threshold, after closing, after opening.}
    \includegraphics[width=\textwidth]{figures/ch2_morphology_steps.png}
    \label{fig:morph_pipeline}
\end{figure}

\subsection{Contour Analysis}

Once a clean binary mask is obtained, \texttt{cv2.findContours} extracts
the outer boundaries of each connected component.  Bounding rectangles
are then computed and used to crop individual objects.  Small contours
(below a configurable area threshold) are discarded as noise.

\paragraph{Justification.}
This full classical pipeline — HSV → Otsu → morphology → contour
extraction — is executed in \textbf{under 10\,ms per frame} on a
standard CPU, making it orders of magnitude cheaper than deploying a
dedicated deep-learning detector such as YOLO for the sole purpose of
isolating objects against a uniform background.

% ────────────────────────────────────────────────────────────
\section{Deep Learning for Agricultural Image Classification}
\label{sec:deep_learning}
% ────────────────────────────────────────────────────────────

\subsection{CNN Fundamentals}

A Convolutional Neural Network (CNN) learns hierarchical features from
images through a sequence of convolution, activation, and pooling layers.
Early layers detect low-level edges; deeper layers capture high-level
patterns such as texture and shape.  A final fully connected (dense)
layer maps the learned features to class probabilities.

\subsection{Transfer Learning}

Training a CNN from scratch requires large datasets and significant
compute time.  \textit{Transfer learning} reuses a model pre-trained on
a large-scale dataset (e.g., ImageNet~\cite{deng2009imagenet}) and
replaces only the final classification head with one tailored to the new
task.  The pre-trained layers — already rich in generic visual
features — are frozen, and only the new head is
trained~\cite{almomen2023date}.  This strategy dramatically reduces
training time and data requirements.

\subsection{Architecture Comparison}

Table~\ref{tab:cnn_comparison} compares three architectures commonly
used for agricultural image classification.

\begin{table}[H]
\centering
\caption{Comparison of CNN architectures for lightweight deployment.}
\label{tab:cnn_comparison}
\begin{tabularx}{\textwidth}{l c c X}
\toprule
\textbf{Model} & \textbf{Params (M)} & \textbf{Top-1 (\%)} &
\textbf{Key Feature} \\
\midrule
ResNet-50~\cite{almomen2023date}
  & 25.6 & 76.1
  & Skip connections; very deep but heavy \\
MobileNetV2~\cite{sandler2018mobilenetv2}
  & 3.4  & 72.0
  & Inverted residuals + depthwise separable convolutions; very lightweight \\
EfficientNet-B0
  & 5.3  & 77.3
  & Compound scaling; excellent accuracy-to-size ratio \\
\bottomrule
\end{tabularx}
\end{table}

\paragraph{Justification.}
MobileNetV2 was selected for this project for three reasons:
\begin{enumerate}[nosep]
    \item \textbf{Size.} At 3.4\,M parameters, it is 7.5$\times$ smaller
          than ResNet-50, fitting comfortably within the 512\,MB RAM
          limit of a free-tier cloud container.
    \item \textbf{Speed.} Depthwise separable convolutions reduce
          multiply–accumulate operations, enabling sub-second inference
          on CPU-only environments (our Dockerfile uses
          \texttt{tensorflow-cpu}).
    \item \textbf{Proven accuracy on fruit data.} Almomen et
          al.~\cite{almomen2023date} demonstrated that MobileNet
          architectures achieve competitive accuracy on date surface
          quality classification, confirming suitability for this domain.
\end{enumerate}

% ────────────────────────────────────────────────────────────
\clearpage
\section{IoT Architectures for Smart Agriculture}
\label{sec:iot}
% ────────────────────────────────────────────────────────────

Modern IoT systems in agriculture follow a layered
\textbf{Edge–Fog–Cloud} architecture~\cite{shi2016edge}.
Table~\ref{tab:iot_protocols} compares the two dominant communication
protocols.

\begin{table}[H]
\centering
\caption{Comparison of IoT communication protocols.}
\label{tab:iot_protocols}
\begin{tabularx}{\textwidth}{l c c X}
\toprule
\textbf{Protocol} & \textbf{Pattern} & \textbf{Overhead} &
\textbf{Typical Use Case} \\
\midrule
MQTT  & Publish/Subscribe & Very low  & Telemetry from constrained
                                         sensors (temperature, humidity) \\
HTTP/REST & Request/Response & Moderate & File uploads, image transfer,
                                          API-based inference \\
\bottomrule
\end{tabularx}
\end{table}

\paragraph{Justification.}
Our system transmits full-resolution images ($\approx$100--300\,KB each)
and expects a structured JSON response from the server.  The
\textbf{request/response} model of HTTP/REST is therefore more natural
than the fire-and-forget semantics of MQTT.  FastAPI was chosen as the
server framework because it provides automatic OpenAPI documentation,
native \texttt{async} support, and built-in data validation via
Pydantic~\cite{fastapi} — all with minimal boilerplate.

The deployment strategy uses \textbf{Docker}
containers~\cite{newman2021building} to encapsulate the Python runtime,
TensorFlow CPU, and the trained model into a single reproducible image.
This ensures that the API behaves identically in development and
production, eliminating ``it works on my machine'' issues.

% ────────────────────────────────────────────────────────────
\section{Generative AI for Operational Decision Support}
\label{sec:genai}
% ────────────────────────────────────────────────────────────

Large Language Models (LLMs) such as GPT and
Gemini~\cite{team2023gemini} have demonstrated strong capabilities in
interpreting structured data and generating natural-language summaries.
In an industrial context, this translates to converting raw production
statistics (e.g., rejection rate, class distribution) into
\textit{actionable managerial recommendations} — a task traditionally
requiring a human quality manager.

\paragraph{Prompt Engineering.}
The quality of LLM output depends heavily on prompt design.  In this
project, the dashboard sends a structured prompt containing:
(i)~numerical statistics from the database,
(ii)~the role instruction (``You are a quality manager''), and
(iii)~an output format specification (bullet-point recommendations).
This approach is known as \textit{role-based prompting} and has been
shown to improve domain-specific response quality.

\paragraph{Risks.}
LLMs are prone to \textit{hallucination} — generating plausible but
factually incorrect statements.  For this reason, generated reports in
our system are presented as \textit{suggestions} requiring human
validation, not as automated commands.

\paragraph{Justification.}
Google Gemini was selected over OpenAI GPT for two practical reasons:
(i)~the Gemini API offers a free tier sufficient for a university
project, and (ii)~the \texttt{google-generativeai} Python SDK integrates
directly with the existing Google Cloud ecosystem.

% ────────────────────────────────────────────────────────────
\section{Comparative Analysis of Related Work}
\label{sec:related_work}
% ────────────────────────────────────────────────────────────

Table~\ref{tab:related_work} summarises representative studies in
date fruit classification and agricultural quality control systems.

\begin{table}[H]
\centering
\caption{Comparative analysis of related work.}
\label{tab:related_work}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Study} & \textbf{Fruit} & \textbf{Detection} &
\textbf{Classifier} & \textbf{Real-time?} &
\textbf{IoT?} & \textbf{LLM?} \\
\midrule
Almomen et al.~\cite{almomen2023date}
  & Dates & None & CNN (VGG, ResNet) & No & No & No \\
Altaheri et al.~\cite{altaheri2019date}
  & Dates & None & Dataset contribution & No & No & No \\
Ouhda et al.~\cite{ouhda2023smart}
  & Dates & YOLO & YOLO + K-Means & Yes & No & No \\
Almutairi et al.~\cite{almutairi2024date}
  & Dates & YOLOv8 & YOLOv8 & Yes & No & No \\
Lipiński et al.~\cite{lipinski2025application}
  & Dates & YOLO/R-CNN & YOLOv8n / ResNet-50 & Yes & No & No \\
\midrule
\textbf{This work}
  & Dates & Classical CV & MobileNetV2 & Yes & Yes & Yes \\
\bottomrule
\end{tabular}%
}
\end{table}

% ────────────────────────────────────────────────────────────
\section{Synthesis and Identified Gaps}
\label{sec:gaps}
% ────────────────────────────────────────────────────────────

The literature review reveals the following observations:

\begin{enumerate}
    \item Most studies on date classification focus exclusively on the
          \textit{model accuracy} and do not address system integration,
          deployment, or data logging.

    \item When object detection is needed, researchers typically employ
          heavy deep-learning detectors (e.g., YOLO), even when the
          background is controlled and classical vision would suffice
          at a fraction of the computational cost.

    \item No existing work — to the best of our knowledge — combines
          \textbf{all five} of the following in a single pipeline:
          \begin{itemize}[nosep]
              \item Classical CV-based object isolation,
              \item Lightweight CNN classification,
              \item Cloud-deployed containerised API,
              \item Persistent IoT data logging, and
              \item LLM-powered quality report generation.
          \end{itemize}
\end{enumerate}

This identified gap directly motivates the system presented in the
following chapters, where each of the five components above is designed,
implemented, and evaluated.


































































% ============================================================
%             CHAPTER 3 – SYSTEM DESIGN & ARCHITECTURE
% ============================================================
\chapter{System Design and Architecture}
\label{ch:design}

This chapter describes the functional and non-functional requirements
derived from the project subject (Section~\ref{sec:requirements}),
the four-layer architecture that satisfies them
(Section~\ref{sec:highlevel}), and the implementation details of each
layer (Sections~\ref{sec:edge}–\ref{sec:container}).

% ────────────────────────────────────────────────────────────
\section{Requirements Analysis}
\label{sec:requirements}
% ────────────────────────────────────────────────────────────

\subsection{Functional Requirements}

Table~\ref{tab:fr} lists the functional requirements traced directly
from the project subject.

\begin{table}[H]
\centering
\caption{Functional requirements.}
\label{tab:fr}
\begin{tabularx}{\textwidth}{l X}
\toprule
\textbf{ID} & \textbf{Description} \\
\midrule
FR-01 & Capture images from a simulated IoT camera and transmit them
        to the cloud API. \\
FR-02 & Detect and isolate individual date fruits from an input image
        using classical computer vision. \\
FR-03 & Classify each isolated fruit as \textit{Fresh} or \textit{Dry}
        using a CNN model. \\
FR-04 & Log every prediction (filename, class, confidence) to a
        persistent database. \\
FR-05 & Visualise production metrics (totals, class distribution)
        in a real-time dashboard. \\
FR-06 & Generate a natural-language quality report using a Large
        Language Model. \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Non-Functional Requirements}

\begin{table}[H]
\centering
\caption{Non-functional requirements.}
\label{tab:nfr}
\begin{tabularx}{\textwidth}{l X}
\toprule
\textbf{ID} & \textbf{Description} \\
\midrule
NFR-01 & Inference latency $< 2$\,s per image (excluding cold start). \\
NFR-02 & Containerised, reproducible deployment via Docker. \\
NFR-03 & Zero-disk image processing (all operations in RAM). \\
NFR-04 & Graceful error handling: network timeouts, missing models,
         database failures. \\
\bottomrule
\end{tabularx}
\end{table}

% ────────────────────────────────────────────────────────────
\section{High-Level Architecture}
\label{sec:highlevel}
% ────────────────────────────────────────────────────────────

The system is organised into four decoupled layers, as shown in
Figure~\ref{fig:arch_ch3}.  Each layer communicates through a well-defined
interface (HTTP or SQL), enabling independent development, testing, and
deployment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/ch3_architecture.png}
    \caption{High-level four-layer architecture of the proposed system.}
    \label{fig:arch_ch3}
\end{figure}

\begin{description}
    \item[Edge Layer] simulates the IoT camera
          (\texttt{iot\_simulation.py}).
    \item[Application Layer] hosts the FastAPI inference service
          (\texttt{api.py}) with the preprocessing
          (\texttt{detection.py}) and CNN model.
    \item[Data Persistence Layer] stores prediction logs in a Supabase
          PostgreSQL database.
    \item[Management Layer] provides a Streamlit dashboard
          (\texttt{dashboard.py}) and LLM-based report generation
          (\texttt{manager.py}).
\end{description}

% ────────────────────────────────────────────────────────────
\section{Edge Layer: IoT Camera Simulation}
\label{sec:edge}
% ────────────────────────────────────────────────────────────

The edge layer is implemented as a standalone Python script that mimics
an industrial camera mounted above a conveyor belt.  It iterates over a
folder of test images, transmitting each one to the cloud API via an
HTTP POST request.  A configurable delay between images simulates the
belt speed.

Listing~\ref{lst:iot_core} shows the core simulation loop.

\begin{lstlisting}[
  language=Python, caption={Core loop of the IoT simulation
  (\texttt{iot\_simulation.py}).}, label={lst:iot_core},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
def simulate(img_paths: list):
    for img in img_paths:
        sleep(2)  # simulates conveyor belt delay
        print(f"Capturing: {img.name}...")
        with open(img, 'rb') as f:
            res = requests.post(
                API_URL + '/upload_and_predict',
                files={"file": f}, timeout=60
            )
            if res.status_code == 200:
                result = res.json()
                label = result['predicted_class']
                conf  = result['confidence']
                print(f"--> [{label}] ({conf:.2f}%)")
\end{lstlisting}

\paragraph{Key design decisions.}
\begin{itemize}[nosep]
    \item \textbf{Timeout of 60\,s:} accommodates the cold-start delay
          of free-tier cloud hosting (Render).
    \item \textbf{Random shuffling} of images before iteration prevents
          class-ordered bias during testing.
    \item \textbf{Connection error handling:} the script catches
          \texttt{ConnectionError} and continues to the next image
          rather than crashing the entire simulation.
\end{itemize}

\clearpage
% ────────────────────────────────────────────────────────────
\section{Application Layer: Cloud API}
\label{sec:api}
% ────────────────────────────────────────────────────────────

The central inference service is built with
FastAPI~\cite{fastapi} and served by Uvicorn.
Figure~\ref{fig:api_flow} illustrates the request lifecycle.

\begin{figure}[H]
    \centering
    % ┌──────────────────────────────────────────────────────┐
    % │  PLACEHOLDER: API request flow diagram                │
    % │  HOW TO CREATE:                                       │
    % │  1. In draw.io, create a horizontal flow:             │
    % │     [Client POST /upload_and_predict]                 │
    % │       -> [Decode image in BytesIO]                    │
    % │       -> [detect_and_crop()]                          │
    % │       -> [MobileNetV2 predict()]                      │
    % │       -> [Return JSON PredictionOut]                  │
    % │              |                                        │
    % │              +-(BackgroundTask)-> [Supabase INSERT]   │
    % │  2. Export as figures/ch3_api_flow.png                │
    % │  3. Uncomment the \includegraphics line below.        │
    % └──────────────────────────────────────────────────────┘
    %
    % \includegraphics[width=0.85\textwidth]{figures/ch3_api_flow.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{2.5cm}
        \textbf{[FIGURE PLACEHOLDER]}\\
        API request lifecycle (POST → detect → classify → respond → log).
    \vspace{2.5cm}}}
    \caption{Request lifecycle of the \texttt{/upload\_and\_predict} endpoint.}
    \label{fig:api_flow}
\end{figure}

Listing~\ref{lst:api_endpoint} shows the prediction endpoint.

\begin{lstlisting}[
  language=Python, caption={Prediction endpoint
  (\texttt{api.py}).}, label={lst:api_endpoint},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
@app.post("/upload_and_predict/", response_model=PredictionOut)
def upload_and_predict(background_tasks: BackgroundTasks,
                       file: UploadFile = File(...)):
    # Zero-disk: decode directly from the byte stream
    img = keras.utils.load_img(
        BytesIO(file.file.read()), target_size=(224, 224)
    )
    image_array = keras.utils.img_to_array(img)
    image_array = tf.expand_dims(image_array, 0)

    predictions = model.predict(image_array, verbose=0)
    score = tf.nn.softmax(predictions[0])

    predicted_class = CLASSES[np.argmax(score)]
    confidence = float(100 * np.max(score))

    # Non-blocking database write
    background_tasks.add_task(
        log_prediction, file.file.name,
        predicted_class, confidence
    )
    return PredictionOut(
        predicted_class=predicted_class,
        confidence=confidence
    )
\end{lstlisting}

\paragraph{Key design decisions.}
\begin{itemize}[nosep]
    \item \textbf{Zero-disk architecture (NFR-03):} the uploaded file
          is read into a \texttt{BytesIO} buffer and never written to
          disk.  This maximises speed and eliminates temporary-file
          cleanup.
    \item \textbf{Asynchronous logging:} database writes are delegated
          to a FastAPI \texttt{BackgroundTask}, so the HTTP response is
          returned immediately after inference completes.
    \item \textbf{Pydantic schema (\texttt{PredictionOut}):} enforces a
          typed JSON contract (\texttt{predicted\_class: str},
          \texttt{confidence: float}), providing automatic validation
          and interactive API documentation via Swagger UI.
\end{itemize}

% ────────────────────────────────────────────────────────────
\section{Data Persistence Layer}
\label{sec:database}
% ─���──────────────────────────────────────────────────────────

Every prediction is persisted in a Supabase-hosted PostgreSQL database.
Table~\ref{tab:db_schema} describes the schema of the \texttt{logs}
table.

\begin{table}[H]
\centering
\caption{Schema of the \texttt{logs} table.}
\label{tab:db_schema}
\begin{tabular}{l l l l}
\toprule
\textbf{Column} & \textbf{Type} & \textbf{Nullable} &
\textbf{Description} \\
\midrule
\texttt{id}         & \texttt{bigint}      & No  & Auto-incremented primary key \\
\texttt{created\_at}& \texttt{timestamptz} & No  & Insertion timestamp (default \texttt{now()}) \\
\texttt{filename}   & \texttt{text}        & Yes & Original image filename \\
\texttt{prediction} & \texttt{text}        & Yes & Predicted class (Fresh / Dry) \\
\texttt{confidence} & \texttt{float8}      & Yes & Confidence score (\%) \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    % ┌──────────────────────────────────────────────────────┐
    % │  PLACEHOLDER: Screenshot of Supabase table            │
    % │  HOW TO CREATE:                                       │
    % │  1. Log in to https://supabase.com/dashboard          │
    % │  2. Open your project → Table Editor → "logs"         │
    % │  3. Take a screenshot showing a few rows of data      │
    % │  4. Save as figures/ch3_supabase_table.png            │
    % │  5. Uncomment the \includegraphics line below.        │
    % └──────────────────────────────────────────────────────┘
    %
    % \includegraphics[width=0.85\textwidth]{figures/ch3_supabase_table.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{2cm}
        \textbf{[FIGURE PLACEHOLDER]}\\
        Screenshot of the Supabase \texttt{logs} table with sample data.
    \vspace{2cm}}}
    \caption{Sample rows from the \texttt{logs} table in Supabase.}
    \label{fig:supabase}
\end{figure}

\paragraph{Justification.}
Supabase was chosen over a self-hosted PostgreSQL instance because it
provides: (i)~a generous free tier (500\,MB), (ii)~a built-in REST API
via PostgREST, and (iii)~row-level security with service-role key
authentication, eliminating the need to manage database infrastructure.

% ────────────────────────────────────────────────────────────
\section{Management Layer: Dashboard and Generative AI}
\label{sec:dashboard}
% ────────────────────────────────────────────────────────────

\subsection{Streamlit Dashboard}

The dashboard (\texttt{dashboard.py}) serves as the operator's control
panel.  It fetches prediction logs from Supabase, computes summary
metrics, and renders interactive visualisations using Plotly.

Listing~\ref{lst:dashboard_metrics} shows the KPI computation.

\begin{lstlisting}[
  language=Python, caption={Dashboard KPI computation
  (\texttt{dashboard.py}).}, label={lst:dashboard_metrics},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
total_count = len(data)
fresh_count = data['prediction'].value_counts()['Fresh']
dry_count   = data['prediction'].value_counts()['Dry']

col1, col2, col3 = st.columns(3)
col1.metric(label='Total',  value=total_count)
col2.metric(label='Fresh',  value=fresh_count)
col3.metric(label='Dry',    value=dry_count)
\end{lstlisting}

\begin{figure}[H]
    \centering
    % ┌──────────────────────────────────────────────────────┐
    % │  PLACEHOLDER: Screenshot of the Streamlit dashboard   │
    % │  HOW TO CREATE:                                       │
    % │  1. Run: streamlit run src/dashboard.py               │
    % │  2. Wait for it to load with real data from Supabase  │
    % │  3. Screenshot the full page (metrics + pie chart)    │
    % │  4. Save as figures/ch3_dashboard.png                 │
    % │  5. Uncomment the \includegraphics line below.        │
    % └──────────────────────────────────────────────────────┘
    %
    % \includegraphics[width=0.85\textwidth]{figures/ch3_dashboard.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{2.5cm}
        \textbf{[FIGURE PLACEHOLDER]}\\
        Screenshot of the Streamlit dashboard showing KPI cards and
        pie chart.
    \vspace{2.5cm}}}
    \caption{Streamlit dashboard with real-time production metrics.}
    \label{fig:dashboard}
\end{figure}

\subsection{LLM-Based Quality Manager}

The reporting module (\texttt{manager.py}) converts raw production
statistics into a professional quality control report.  The
\texttt{QualityManager} class classifies the current batch severity
based on the loss rate and constructs a role-based prompt that is
sent to a text-generation model.

Listing~\ref{lst:severity} shows the severity classification logic.

\begin{lstlisting}[
  language=Python, caption={Severity classification and prompt
  construction (\texttt{manager.py}).}, label={lst:severity},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
# Classify severity based on industrial thresholds
if data.loss_rate > 15:
    severity = "CRITICAL"
elif data.loss_rate > 5:
    severity = "WARNING"
else:
    severity = "ACCEPTABLE"

prompt = f"""Quality Control Report for Date Packaging Co.
PRODUCTION BATCH DATA:
- Total Units Processed: {data.fresh + data.rotten}
- Grade 1 (Fresh): {data.fresh} units
- Grade 3 (Rejected): {data.rotten} units
- Loss Rate: {data.loss_rate:.2f}%
- Severity Status: {severity}
..."""
\end{lstlisting}

The prompt is designed with three elements: (i)~numerical context
(batch statistics), (ii)~a role instruction (quality manager), and
(iii)~an output format specification (executive summary, root cause
analysis, corrective actions).  A template-based
\texttt{\_generate\_fallback\_report()} method ensures the system still
produces a usable report even if the LLM call fails.

% ────────────────────────────────────────────────────────────
\section{Containerisation and Deployment}
\label{sec:container}
% ────────────────────────────────────────────────────────────

The API is packaged as a Docker image for reproducible deployment.
Listing~\ref{lst:dockerfile} shows the complete Dockerfile.

\begin{lstlisting}[
  language=bash, caption={Dockerfile for the inference API.},
  label={lst:dockerfile},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, commentstyle=\color{green!50!black},
  frame=single]
FROM mambaorg/micromamba:1.5-jammy
WORKDIR /app
COPY environment.yml .
RUN micromamba install --yes \
  --name base -f environment.yml \
  && micromamba clean --all --yes
ARG MAMBA_DOCKERFILE_ACTIVATE=1
COPY . .
ENTRYPOINT ["micromamba", "run", "-n", "base", \
  "uvicorn", "src.api:app", \
  "--host", "0.0.0.0", "--port", "8000"]
\end{lstlisting}

\paragraph{Key design decisions.}
\begin{itemize}[nosep]
    \item \textbf{Micromamba over pip:} Micromamba resolves
          \texttt{conda-forge} and \texttt{pip} dependencies in a
          single step, producing a smaller and more reliable image than
          a standard \texttt{python:3.11} base with pip-only
          installs~\cite{newman2021building}.
    \item \textbf{Pinned versions (\texttt{environment.yml}):} every
          package is version-locked (e.g., \texttt{tensorflow-cpu==2.20.0},
          \texttt{keras==3.10.0}) to guarantee reproducibility across
          builds.
    \item \textbf{\texttt{tensorflow-cpu}:} the full GPU build of
          TensorFlow exceeds 1.5\,GB.  Since Render's free tier provides
          no GPU, using the CPU variant cuts the image size by
          $\approx$60\%.
\end{itemize}

The image is deployed on \textbf{Render} (free tier).  On each
\texttt{git push} to the \texttt{main} branch, Render automatically
rebuilds the Docker image and redeploys the service — providing a
basic CI/CD pipeline at zero cost.

\begin{figure}[H]
    \centering
    % ┌──────────────────────────────────────────────────────┐
    % │  PLACEHOLDER: Screenshot of Render deployment         │
    % │  HOW TO CREATE:                                       │
    % │  1. Log in to https://dashboard.render.com            │
    % │  2. Open your "csqa-cnn-api" service                  │
    % │  3. Screenshot the dashboard showing status "Live"    │
    % │  4. Save as figures/ch3_render.png                    │
    % │  5. Uncomment the \includegraphics line below.        │
    % └──────────────────────────────────────────────────────┘
    %
    % \includegraphics[width=0.85\textwidth]{figures/ch3_render.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{2cm}
        \textbf{[FIGURE PLACEHOLDER]}\\
        Render dashboard showing the deployed API service.
    \vspace{2cm}}}
    \caption{Render deployment dashboard for the cloud API.}
    \label{fig:render}
\end{figure}



































































% ============================================================
%         CHAPTER 4 – PREPROCESSING & DETECTION PIPELINE
% ============================================================
\chapter{Intelligent Preprocessing and Object Detection}

\section{Overview of the Two-Stage Pipeline}
% Why classical CV before DL: computational efficiency, resource savings.

\section{Image Decoding and Colour-Space Transformation}
% - OpenCV imdecode from byte buffer.
% - BGR → HSV conversion; rationale for using the Saturation channel.

\section{Foreground Mask Generation}
\subsection{Gaussian Blur for Noise Reduction}
\subsection{Otsu's Adaptive Thresholding}
\subsection{Morphological Closing and Opening}
% Kernel size (7×7), iterations; healing internal textures.

\section{Scene-Aware Adaptive Logic}
\subsection{Macro Mode (Single Object, Coverage > 50\%)}
\subsection{Conveyor Mode (Multiple Objects)}
% Contour area coverage ratio as the switching criterion.

\section{Artifact Rejection}
% - Dark-crop filtering (mean intensity < 20).
% - Minimum object area threshold (3000 px²).
% - Padded cropping for context preservation.

\section{Algorithm Summary}
% Pseudocode (algorithm environment) for detect_and_crop().


% ============================================================
%         CHAPTER 5 – DATASET & MODEL TRAINING
% ============================================================
\chapter{Dataset Preparation and Model Training}

\section{Dataset Description}
% - Source: Augmented Date Fruit Dataset.
% - Original structure: Variety / Size / Grade.
% - Total image count per class.

\section{Data Reorganisation}
% - Mapping Grade-1 → Fresh, Grade-3 → Rotten/Dry.
% - Renaming convention: Variety\_Size\_Grade\_filename.
% - Reference to reorganization.py.

\section{Train / Test Split}
% - 80/20 stratified split.
% - Shuffling with reproducibility considerations.
% - Reference to splitting.py.

\section{Data Loading and Augmentation}
% - Keras image_dataset_from_directory.
% - 20\% validation split from training set.
% - Image resizing to 224 × 224.
% - Batch size = 32; seed = 123.

\section{Model Architecture: MobileNetV2}
\subsection{Inverted Residual Blocks and Depthwise Separable Convolutions}
\subsection{Transfer Learning Strategy}
% - Frozen base (ImageNet weights).
% - Custom head: GlobalAveragePooling2D → Dense(2, softmax).

\section{Training Configuration}
% - Optimizer: Adam.
% - Loss: Sparse Categorical Cross-Entropy.
% - Class-weight balancing (sklearn compute_class_weight).
% - Epochs: 5 (justification: frozen base converges fast).

\section{Model Serialisation}
% - Saved as mobilenet_dates.keras.
% - Loaded at API startup.


% ============================================================
%        CHAPTER 6 – EXPERIMENTATION & RESULTS
% ============================================================
\chapter{Experimentation and Results}

\section{Experimental Setup}
% - Hardware: Google Colab (T4 GPU for training); Render free tier for serving.
% - Software versions: Python 3.11, TensorFlow 2.20, Keras 3.10.

\section{Training Results}
\subsection{Accuracy and Loss Curves}
% Figures: training vs.\ validation accuracy; training vs.\ validation loss.
\subsection{Convergence Analysis}

\section{Test-Set Evaluation}
\subsection{Confusion Matrix}
\subsection{Precision, Recall, and F1-Score}
% Per-class and macro-averaged.
\subsection{Discussion of Misclassifications}

\section{Preprocessing Pipeline Evaluation}
% - Number of correctly detected objects vs.\ ground truth.
% - Artifact rejection rate.
% - Qualitative examples (figure grid).

\section{End-to-End System Performance}
% - API latency benchmarks (cold start vs.\ warm).
% - Throughput: images per minute.
% - IoT simulation log analysis.

\section{Generative AI Report Quality}
% - Example generated report.
% - Qualitative assessment: relevance, actionability, hallucination check.

\section{Comparison with Related Work}
% Table comparing accuracy, model size, and latency with similar studies.


% ============================================================
%         CHAPTER 7 – DISCUSSION
% ============================================================
\chapter{Discussion}

\section{Interpretation of Results}
% - Why MobileNetV2 with transfer learning achieves high accuracy on this task.
% - Effectiveness of the classical CV pre-filter in reducing false positives.

\section{Strengths of the Proposed System}
% - Modularity (independent layers).
% - Dual-mode detection without code changes.
% - Zero-disk architecture for speed and security.
% - LLM-augmented decision support.

\section{Limitations and Threats to Validity}
% - Binary classification only (no multi-grade or multi-fruit).
% - Simulated IoT (no real camera, no physical actuator).
% - Cold-start latency on free-tier hosting.
% - LLM hallucination risk in quality reports.
% - Dataset bias (augmented images may not reflect field conditions).

\section{Lessons Learned}
% Engineering insights: dependency pinning, async logging, Docker layering.


% ============================================================
%         CHAPTER 8 – CONCLUSION & FUTURE WORK
% ============================================================
\chapter{Conclusion and Future Work}

\section{Summary of Contributions}
% 1. Two-stage (classical CV + CNN) pipeline.
% 2. Full-stack microservices IIoT architecture.
% 3. LLM-powered quality manager.

\section{Answers to Research Objectives}
% Map each objective from Chapter 1 to the achieved results.

\section{Future Work}
% - Multi-class grading (Grade 1, 2, 3).
% - Real hardware deployment (Raspberry Pi + camera module).
% - On-device inference with TFLite quantisation.
% - Anomaly detection with autoencoders for unknown defects.
% - Reinforcement learning for actuator control (reject arm).
% - Fine-tuning an open-source LLM to reduce API dependency.


% ============================================================
%                   BIBLIOGRAPHY
% ============================================================
\printbibliography[heading=bibintoc, title={References}]


% ============================================================
%                   APPENDICES
% ============================================================
\appendix

\chapter{Source Code Listings}
% Key extracts: detection.py, api.py, compile.py, iot_simulation.py.

\chapter{Dockerfile and Environment Configuration}
% Full Dockerfile and environment.yml.

\chapter{Sample Generative AI Quality Report}
% A complete example output from Google Gemini.

\chapter{Dataset Sample Images}
% Grid of Fresh vs.\ Dry examples before and after preprocessing.

\end{document}
