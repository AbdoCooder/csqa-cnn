% ============================================================
%  IoT-Based Intelligent Harvest Sorting & Quality Analysis
%  Academic Research Paper – Outline
% ============================================================

\documentclass[12pt, a4paper, oneside]{report}

% ── Packages ────────────────────────────────────────────────
\usepackage{svg}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{geometry}
  \geometry{margin=2.5cm}
\usepackage{setspace}
  \onehalfspacing
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amsmath, amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{hyperref}
  \hypersetup{colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue}
\usepackage[backend=biber, style=ieee]{biblatex}
  \addbibresource{references.bib}
\usepackage{acronym}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
  \usetikzlibrary{shapes, arrows.meta, positioning}

% ── Meta ────────────────────────────────────────────────────
\title{
  \textbf{IoT-Based Intelligent Harvest Sorting\\
  and Quality Analysis System}\\[6pt]
  \large A Microservices Architecture Integrating Classical Computer Vision,\\
  Deep Learning, and Generative AI for Automated Crop Quality Control
}
\author{
  \textbf{Author Name}\\
  Department of Software Engineering\\
  University Name\\
  \texttt{author@university.edu}
}
\date{Academic Year 2025 -- 2026}

% ============================================================
\begin{document}

\pagestyle{plain}

\maketitle
\newpage

% ── Abstract ────────────────────────────────────────────────
\begin{abstract}
  % ≈ 250 words
  % 1. Context & motivation
  % 2. Problem statement
  % 3. Proposed approach (two-stage pipeline, microservices architecture)
  % 4. Key results (accuracy, latency, throughput)
  % 5. Conclusion & significance
  \textit{(To be written after experimentation.)}
\end{abstract}

\tableofcontents
\listoffigures
\listoftables

% % ── Acronyms ────────────────────────────────────────────────
\chapter*{List of Acronyms}
\addcontentsline{toc}{chapter}{List of Acronyms}
\begin{table}[h!]
    \centering
    \large
    \label{tab:acronyms}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Acronym} & \textbf{Full Term} \\ \hline
        CNN & Convolutional Neural Network \\ \hline
        API & Application Programming Interface \\ \hline
        IoT & Internet of Things \\ \hline
        IIoT & Industrial Internet of Things \\ \hline
        HSV & Hue, Saturation, Value \\ \hline
        LLM & Large Language Model \\ \hline
        REST & Representational State Transfer \\ \hline
        GAP & Global Average Pooling \\ \hline
    \end{tabular}
\end{table}

% ============================================================
%                   CHAPTER 1 – INTRODUCTION
% ============================================================
\chapter{General Introduction}
\label{ch:introduction}

% ────────────────────────────────────────────────────────────
\section{Context and Motivation}
\label{sec:context}
% ────────────────────────────────────────────────────────────

The date palm (\textit{Phoenix dactylifera} L.) is one of the oldest cultivated
fruit trees, with significant economic and cultural importance across the
Middle East and North Africa.

According to the Food and Agriculture Organization of the United Nations
(FAO), global date production reached approximately
\textbf{10.09 million tonnes} in \textbf{2024}, with
\textbf{Saudi Arabia}, \textbf{Egypt}, and \textbf{Algeria}
being the leading producers~\cite{fao_dates_production}.

Despite their economic value, dates are highly susceptible to quality
degradation caused by fungal infection, insect infestation, excessive
moisture, and mechanical damage during harvesting and
transportation~\cite{almomen2023date}.
Post-harvest losses in the date sector are estimated at
\textbf{30\%} of the total annual yield in several producing
regions~\cite{fao_food_loss}.

In traditional packing facilities, quality grading is still predominantly
performed by human inspectors who visually assess each fruit for colour,
texture, size, and the presence of defects.  This manual process is
inherently slow, subjective, and non-reproducible: two different operators
may assign different grades to the same fruit, and fatigue significantly
degrades accuracy over long shifts~\cite{almomen2023date}.

The convergence of the Internet of Things (IoT), computer vision, and deep
learning offers a promising path toward automating this process.  Modern
lightweight Convolutional Neural Networks (CNNs) such as MobileNetV2 can
classify images with high accuracy while remaining deployable on
resource-constrained environments~\cite{sandler2018mobilenetv2}.
When combined with classical image processing for object isolation and
cloud-based inference services, a complete, non-destructive, and scalable
quality control pipeline becomes feasible.

This project is therefore motivated by the need for an
\textbf{end-to-end intelligent system} that can:
\begin{enumerate}
    \item detect and isolate individual date fruits from a conveyor-belt
          camera feed,
    \item classify each fruit's quality in real time, and
    \item log, visualise, and \emph{interpret} the production data
          through generative AI.
\end{enumerate}

% ────────────────────────────────────────────────────────────
\section{Problem Statement}
\label{sec:problem}
% ────────────────────────────────────────────────────────────

Although several studies have explored CNN-based classification of date
fruits~\cite{almomen2023date, altaheri2019date}, most focus exclusively on
the classification task in isolation.  The broader engineering challenge of
integrating detection, classification, data persistence, and operational
decision support into a single deployable system remains largely
unaddressed.  The following specific gaps motivate the present work:

\begin{enumerate}
    \item \textbf{Subjectivity of manual grading.}
    As discussed in Section~\ref{sec:context}, human inspectors produce
    inconsistent grades due to lighting, fatigue, and individual perception.

    \item \textbf{Computational cost of detection-only deep learning
    pipelines.}
    End-to-end object-detection models such as YOLO~\cite{redmon2016yolo}
    achieve high accuracy but require significant GPU resources.  For
    scenarios where the background is controlled (e.g., a white conveyor
    belt), classical computer vision techniques can isolate objects at a
    fraction of the cost.

    \item \textbf{Absence of integrated quality control workflows.}
    Existing research typically stops at reporting a classification accuracy
    score.  A production-grade system must also handle image ingestion from
    an IoT device, persist predictions in a database, and surface
    actionable insights — not just raw numbers — to operations managers.
\end{enumerate}

The central question addressed in this work can be formulated as:

\begin{quote}
\textit{How can a lightweight, cloud-deployable system be designed to
automatically detect, classify, and log the quality of date fruits in real
time, while providing AI-generated operational insights to production
managers?}
\end{quote}

% ────────────────────────────────────────────────────────────
\section{Objectives}
\label{sec:objectives}
% ────────────────────────────────────────────────────────────

To answer the research question stated above, four operational objectives
are defined:

\begin{enumerate}
    \item[\textbf{O1.}] \textbf{Design a two-stage vision pipeline} that
    combines classical computer vision (colour-space transformation,
    Otsu's thresholding, morphological operations) for object isolation
    with a pre-trained MobileNetV2 CNN for quality classification.

    \item[\textbf{O2.}] \textbf{Implement a containerised cloud API}
    using FastAPI and Docker, capable of receiving images over HTTP,
    performing inference entirely in memory, and returning predictions
    with sub-second latency.

    \item[\textbf{O3.}] \textbf{Simulate an IoT edge device} that mimics
    a conveyor-mounted industrial camera, continuously capturing and
    transmitting images to the cloud service with built-in network
    resilience.

    \item[\textbf{O4.}] \textbf{Persist inspection data and generate
    AI-driven quality reports} by logging every prediction to a
    PostgreSQL database (Supabase) and integrating a Large Language
    Model (Google Gemini) to interpret production trends and generate
    managerial recommendations.
\end{enumerate}

% ────────────────────────────────────────────────────────────
\section{Proposed Approach}
\label{sec:approach}
% ────────────────────────────────────────────────────────────

The proposed system follows a four-layer microservices architecture,
illustrated in Figure~\ref{fig:high_level_arch}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.88\textwidth]{figures/ch1_architecture.png}
    \caption{High-level architecture of the proposed system.}
    \label{fig:high_level_arch}
\end{figure}

\begin{itemize}[nosep]
    \item \textbf{Edge Layer} — IoT camera simulation
          (\texttt{iot\_simulation.py}).
    \item \textbf{Application Layer} — FastAPI inference service
          (\texttt{api.py}) with two-stage vision pipeline.
    \item \textbf{Data Persistence Layer} — Supabase PostgreSQL for
          prediction logging.
    \item \textbf{Management Layer} — Streamlit dashboard with
          LLM-based report generation (\texttt{manager.py}).
\end{itemize}

Each layer is described in detail in Chapter~\ref{ch:design}.  The core
technical contribution — the two-stage vision pipeline combining classical
CV for object isolation with a MobileNetV2 CNN for classification — is
presented in Chapters~\ref{ch:preprocessing} and~\ref{ch:training}.

% ────────────────────────────────────────────────────────────
\section{Scope and Limitations}
\label{sec:scope}
% ────────────────────────────────────────────────────────────

The boundaries of this work were defined to ensure feasibility within a
single-semester Master's module:

\paragraph{In scope.}
\begin{itemize}
    \item Binary classification: \textit{Fresh} (Grade~1) versus
          \textit{Dry/Dry} (Grade~3).
    \item A simulated IoT camera client (software-based, no physical
          hardware).
    \item A single commodity: date fruits, using the
          \textit{Augmented Date Fruit Dataset}.
    \item Cloud deployment on a free-tier platform (Render).
    \item LLM-assisted report generation via the Google Gemini API.
\end{itemize}

\paragraph{Out of scope.}
\begin{itemize}
    \item Multi-class grading (e.g., Grade~1 / Grade~2 / Grade~3).
    \item Physical hardware integration (Raspberry Pi, industrial camera,
          or robotic rejection arm).
    \item On-device edge inference (e.g., TensorFlow Lite quantisation).
    \item Fine-tuning or hosting a local LLM.
\end{itemize}

\paragraph{Acknowledged limitations.}
\begin{itemize}
    \item The augmented dataset may not fully represent the variability
          encountered in real-world packing lines (dust, partial
          occlusion, conveyor vibration).
    \item Free-tier cloud hosting introduces cold-start latency of
          up to 30--60 seconds after periods of inactivity.
    \item LLM-generated reports are subject to hallucination and must be
          reviewed by a human operator before acting on recommendations.
\end{itemize}

% ────────────────────────────────────────────────────────────
\section{Document Structure}
\label{sec:structure}
% ────────────────────────────────────────────────────────────

The remainder of this report is organised as follows:

\begin{description}
    \item[Chapter~\ref{ch:literature}] reviews the state of the art in
    post-harvest quality inspection, classical computer vision,
    CNN-based classification, IoT architectures for agriculture, and
    the use of large language models for operational decision support.

    \item[Chapter~\ref{ch:design}] presents the system design, including
    the microservices architecture, cloud API structure, database schema,
    and dashboard design.

    \item[Chapter~\ref{ch:preprocessing}] details the classical computer
    vision preprocessing pipeline used for object detection and isolation.

    \item[Chapter~\ref{ch:training}] describes dataset preparation,
    the MobileNetV2 transfer-learning strategy, and the training
    configuration.

    \item[Chapter~\ref{ch:results}] reports experimental results,
    including classification metrics, system latency benchmarks, and a
    qualitative assessment of LLM-generated reports.

    \item[Chapter~\ref{ch:discussion}] provides a critical discussion of
    the results, strengths, and limitations.

    \item[Chapter~\ref{ch:conclusion}] concludes with a summary of
    contributions and directions for future work.
\end{description}

% ============================================================
%                 CHAPTER 2 – LITERATURE REVIEW
% ============================================================
\chapter{Literature Review and State of the Art}
\label{ch:literature}

This chapter reviews the key disciplines that underpin the proposed system:
post-harvest inspection practices, classical computer vision,
CNN-based image classification, IoT architectures, and the emerging use
of generative AI for operational decision support.  Each section concludes
with a justification of the technology selected for this project.
A comparative table of related work and a synthesis of identified gaps
close the chapter.

% ────────────────────────────────────────────────────────────
\section{Post-Harvest Quality Inspection}
\label{sec:postharvest}
% ────────────────────────────────────────────────────────────

As introduced in Section~\ref{sec:context}, manual date grading suffers
from subjectivity, fatigue-induced errors, and lack of
traceability~\cite{almomen2023date}.  Instrumental alternatives
(colourimeters, refractometers) offer objectivity but are destructive
and unsuitable for high-throughput conveyor
lines~\cite{szeliski2022computer}.  These limitations motivate our
choice of a \textit{non-destructive, vision-based} pipeline that
produces a persistent digital log for every inspected fruit.

% ────────────────────────────────────────────────────────��───
\clearpage
\section{Classical Computer Vision for Object Segmentation}
\label{sec:classical_cv}
% ────────────────────────────────────────────────────────────

Before a classifier can operate, individual objects must be isolated
from the background.  Classical computer vision provides lightweight
algorithms well-suited to \textit{controlled environments} such as
industrial conveyor belts with uniform backgrounds.

\subsection{Colour-Space Transformations}

The RGB colour model is sensitive to illumination changes.  Converting
an image to the \textbf{HSV} (Hue, Saturation, Value) space decouples
chromatic content from brightness, making it easier to distinguish
coloured objects from grey shadows~\cite{szeliski2022computer}.
Table~\ref{tab:colour_spaces} summarises the three most common colour
spaces used in agricultural vision.

\begin{table}[H]
\centering
\caption{Comparison of colour spaces for fruit segmentation.}
\label{tab:colour_spaces}
\begin{tabularx}{\textwidth}{l X X}
\toprule
\textbf{Space} & \textbf{Advantage} & \textbf{Limitation} \\
\midrule
RGB   & Native camera format, no conversion needed
      & Highly sensitive to lighting changes \\
HSV   & Separates colour (H) from intensity (V); robust shadow rejection
      & Hue wraps around at 0°/360° \\
L*a*b* & Perceptually uniform; good for colour-distance metrics
       & Higher computational cost \\
\bottomrule
\end{tabularx}
\end{table}

\paragraph{Justification.}
Our system converts images to HSV and operates on the
\textbf{Saturation channel} specifically.  Date fruits — whether fresh
or dry — are chromatically rich (high S), whereas conveyor-belt shadows
are achromatic (low S).  This single-channel approach eliminates shadows
without the added cost of L*a*b* conversion.

\subsection{Thresholding Techniques}

Thresholding converts a grey-scale image into a binary mask.  Fixed
(global) thresholds fail when lighting conditions vary.
\textbf{Otsu's method}~\cite{otsu1979threshold} computes the optimal
threshold automatically by maximising the inter-class variance between
foreground and background pixels, making it adaptive to each frame.

\subsection{Morphological Operations}

Binary masks produced by thresholding often contain holes (caused by
fruit textures) and noise (small white specks).
\textit{Morphological closing} — a dilation followed by an erosion —
fills internal gaps, while \textit{morphological opening} — an erosion
followed by a dilation — removes small noise.  Together, they produce
a clean, solid foreground mask suitable for contour
detection~\cite{szeliski2022computer}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/ch2_morphology_steps.png}
    \caption{Visualisation of the foreground mask creation pipeline:
      Saturation channel, Otsu threshold, after closing, after opening.}
    \label{fig:morph_pipeline}
\end{figure}

\subsection{Contour Analysis}

Once a clean binary mask is obtained, \texttt{cv2.findContours} extracts
the outer boundaries of each connected component.  Bounding rectangles
are then computed and used to crop individual objects.  Small contours
(below a configurable area threshold) are discarded as noise.

\paragraph{Justification.}
This full classical pipeline — HSV → Otsu → morphology → contour
extraction — is executed in \textbf{under 10\,ms per frame} on a
standard CPU, making it orders of magnitude cheaper than deploying a
dedicated deep-learning detector such as YOLO for the sole purpose of
isolating objects against a uniform background.

% ────────────────────────────────────────────────────────────
\section{Deep Learning for Agricultural Image Classification}
\label{sec:deep_learning}
% ────────────────────────────────────────────────────────────

\subsection{CNN Fundamentals}

A Convolutional Neural Network (CNN) learns hierarchical features from
images through a sequence of convolution, activation, and pooling layers.
Early layers detect low-level edges; deeper layers capture high-level
patterns such as texture and shape.  A final fully connected (dense)
layer maps the learned features to class probabilities.

\subsection{Transfer Learning}

Training a CNN from scratch requires large datasets and significant
compute time.  \textit{Transfer learning} reuses a model pre-trained on
a large-scale dataset (e.g., ImageNet~\cite{deng2009imagenet}) and
replaces only the final classification head with one tailored to the new
task.  The pre-trained layers — already rich in generic visual
features — are frozen, and only the new head is
trained~\cite{almomen2023date}.  This strategy dramatically reduces
training time and data requirements.

\subsection{Architecture Comparison}

Table~\ref{tab:cnn_comparison} compares three architectures commonly
used for agricultural image classification.

\begin{table}[H]
\centering
\caption{Comparison of CNN architectures for lightweight deployment.}
\label{tab:cnn_comparison}
\begin{tabularx}{\textwidth}{l c c X}
\toprule
\textbf{Model} & \textbf{Params (M)} & \textbf{Top-1 (\%)} &
\textbf{Key Feature} \\
\midrule
ResNet-50~\cite{almomen2023date}
  & 25.6 & 76.1
  & Skip connections; very deep but heavy \\
MobileNetV2~\cite{sandler2018mobilenetv2}
  & 3.4  & 72.0
  & Inverted residuals + depthwise separable convolutions; very lightweight \\
EfficientNet-B0
  & 5.3  & 77.3
  & Compound scaling; excellent accuracy-to-size ratio \\
\bottomrule
\end{tabularx}
\end{table}

\paragraph{Justification.}
MobileNetV2 was selected for this project for three reasons:
\begin{enumerate}[nosep]
    \item \textbf{Size.} At 3.4\,M parameters, it is 7.5$\times$ smaller
          than ResNet-50, fitting comfortably within the 512\,MB RAM
          limit of a free-tier cloud container.
    \item \textbf{Speed.} Depthwise separable convolutions reduce
          multiply–accumulate operations, enabling sub-second inference
          on CPU-only environments (our Dockerfile uses
          \texttt{tensorflow-cpu}).
    \item \textbf{Proven accuracy on fruit data.} Almomen et
          al.~\cite{almomen2023date} demonstrated that MobileNet
          architectures achieve competitive accuracy on date surface
          quality classification, confirming suitability for this domain.
\end{enumerate}

% ────────────────────────────────────────────────────────────
\clearpage
\section{IoT Architectures for Smart Agriculture}
\label{sec:iot}
% ────────────────────────────────────────────────────────────

Modern IoT systems in agriculture follow a layered
\textbf{Edge–Fog–Cloud} architecture~\cite{shi2016edge}.
Table~\ref{tab:iot_protocols} compares the two dominant communication
protocols.

\begin{table}[H]
\centering
\caption{Comparison of IoT communication protocols.}
\label{tab:iot_protocols}
\begin{tabularx}{\textwidth}{l c c X}
\toprule
\textbf{Protocol} & \textbf{Pattern} & \textbf{Overhead} &
\textbf{Typical Use Case} \\
\midrule
MQTT  & Publish/Subscribe & Very low  & Telemetry from constrained
                                         sensors (temperature, humidity) \\
HTTP/REST & Request/Response & Moderate & File uploads, image transfer,
                                          API-based inference \\
\bottomrule
\end{tabularx}
\end{table}

\paragraph{Justification.}
Our system transmits full-resolution images ($\approx$100--300\,KB each)
and expects a structured JSON response from the server.  The
\textbf{request/response} model of HTTP/REST is therefore more natural
than the fire-and-forget semantics of MQTT.  FastAPI was chosen as the
server framework because it provides automatic OpenAPI documentation,
native \texttt{async} support, and built-in data validation via
Pydantic~\cite{fastapi} — all with minimal boilerplate.
The containerisation and deployment strategy is detailed in
Section~\ref{sec:container}.

% ────────────────────────────────────────────────────────────
\section{Generative AI for Operational Decision Support}
\label{sec:genai}
% ────────────────────────────────────────────────────────────

Large Language Models (LLMs) such as GPT and
Gemini~\cite{team2023gemini} have demonstrated strong capabilities in
interpreting structured data and generating natural-language summaries.
In an industrial context, this translates to converting raw production
statistics (e.g., rejection rate, class distribution) into
\textit{actionable managerial recommendations} — a task traditionally
requiring a human quality manager.

\paragraph{Prompt Engineering.}
The quality of LLM output depends heavily on prompt design.  In this
project, the dashboard sends a structured prompt containing:
(i)~numerical statistics from the database,
(ii)~the role instruction (``You are a quality manager''), and
(iii)~an output format specification (bullet-point recommendations).
This approach is known as \textit{role-based prompting} and has been
shown to improve domain-specific response quality.

\paragraph{Risks.}
LLMs are prone to \textit{hallucination} — generating plausible but
factually incorrect statements.  For this reason, generated reports in
our system are presented as \textit{suggestions} requiring human
validation, not as automated commands.

\paragraph{Justification.}
Google Gemini was selected over OpenAI GPT for two practical reasons:
(i)~the Gemini API offers a free tier sufficient for a university
project, and (ii)~the \texttt{google-generativeai} Python SDK integrates
directly with the existing Google Cloud ecosystem.

% ────────────────────────────────────────────────────────────
\section{Comparative Analysis of Related Work}
\label{sec:related_work}
% ────────────────────────────────────────────────────────────

Table~\ref{tab:related_work} summarises representative studies in
date fruit classification and agricultural quality control systems.

\begin{table}[H]
\centering
\caption{Comparative analysis of related work.}
\label{tab:related_work}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c}
\toprule
\textbf{Study} & \textbf{Fruit} & \textbf{Detection} &
\textbf{Classifier} & \textbf{Real-time?} &
\textbf{IoT?} & \textbf{LLM?} \\
\midrule
Almomen et al.~\cite{almomen2023date}
  & Dates & None & CNN (VGG, ResNet) & No & No & No \\
Altaheri et al.~\cite{altaheri2019date}
  & Dates & None & Dataset contribution & No & No & No \\
Ouhda et al.~\cite{ouhda2023smart}
  & Dates & YOLO & YOLO + K-Means & Yes & No & No \\
Almutairi et al.~\cite{almutairi2024date}
  & Dates & YOLOv8 & YOLOv8 & Yes & No & No \\
Lipiński et al.~\cite{lipinski2025application}
  & Dates & YOLO/R-CNN & YOLOv8n / ResNet-50 & Yes & No & No \\
\midrule
\textbf{This work}
  & Dates & Classical CV & MobileNetV2 & Yes & Yes & Yes \\
\bottomrule
\end{tabular}%
}
\end{table}

% ────────────────────────────────────────────────────────────
\section{Synthesis and Identified Gaps}
\label{sec:gaps}
% ────────────────────────────────────────────────────────────

The literature review reveals the following observations:

\begin{enumerate}
    \item Most studies on date classification focus exclusively on the
          \textit{model accuracy} and do not address system integration,
          deployment, or data logging.

    \item When object detection is needed, researchers typically employ
          heavy deep-learning detectors (e.g., YOLO), even when the
          background is controlled and classical vision would suffice
          at a fraction of the computational cost.

    \item No existing work — to the best of our knowledge — combines
          \textbf{all five} of the following in a single pipeline:
          \begin{itemize}[nosep]
              \item Classical CV-based object isolation,
              \item Lightweight CNN classification,
              \item Cloud-deployed containerised API,
              \item Persistent IoT data logging, and
              \item LLM-powered quality report generation.
          \end{itemize}
\end{enumerate}

This identified gap directly motivates the system presented in the
following chapters, where each of the five components above is designed,
implemented, and evaluated.

% ============================================================
%             CHAPTER 3 – SYSTEM DESIGN & ARCHITECTURE
% ============================================================
\chapter{System Design and Architecture}
\label{ch:design}

This chapter describes the functional and non-functional requirements
derived from the project subject (Section~\ref{sec:requirements}),
the four-layer architecture that satisfies them
(Section~\ref{sec:highlevel}), and the implementation details of each
layer (Sections~\ref{sec:edge}–\ref{sec:container}).

% ────────────────────────────────────────────────────────────
\section{Requirements Analysis}
\label{sec:requirements}
% ────────────────────────────────────────────────────────────

\subsection{Functional Requirements}

Table~\ref{tab:fr} lists the functional requirements traced directly
from the project subject.

\begin{table}[H]
\centering
\caption{Functional requirements.}
\label{tab:fr}
\begin{tabularx}{\textwidth}{l X}
\toprule
\textbf{ID} & \textbf{Description} \\
\midrule
FR-01 & Capture images from a simulated IoT camera and transmit them
        to the cloud API. \\
FR-02 & Detect and isolate individual date fruits from an input image
        using classical computer vision. \\
FR-03 & Classify each isolated fruit as \textit{Fresh} or \textit{Dry}
        using a CNN model. \\
FR-04 & Log every prediction (filename, class, confidence) to a
        persistent database. \\
FR-05 & Visualise production metrics (totals, class distribution)
        in a real-time dashboard. \\
FR-06 & Generate a natural-language quality report using a Large
        Language Model. \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Non-Functional Requirements}

\begin{table}[H]
\centering
\caption{Non-functional requirements.}
\label{tab:nfr}
\begin{tabularx}{\textwidth}{l X}
\toprule
\textbf{ID} & \textbf{Description} \\
\midrule
NFR-01 & Inference latency $< 2$\,s per image (excluding cold start). \\
NFR-02 & Containerised, reproducible deployment via Docker. \\
NFR-03 & Zero-disk image processing (all operations in RAM). \\
NFR-04 & Graceful error handling: network timeouts, missing models,
         database failures. \\
\bottomrule
\end{tabularx}
\end{table}

% ────────────────────────────────────────────────────────────
\section{High-Level Architecture}
\label{sec:highlevel}
% ────────────────────────────────────────────────────────────

The system is organised into four decoupled layers, as shown in
Figure~\ref{fig:arch_ch3}.  Each layer communicates through a well-defined
interface (HTTP or SQL), enabling independent development, testing, and
deployment.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/ch3_architecture.png}
    \caption{High-level four-layer architecture of the proposed system.}
    \label{fig:arch_ch3}
\end{figure}

\begin{description}
    \item[Edge Layer] simulates the IoT camera
          (\texttt{iot\_simulation.py}).
    \item[Application Layer] hosts the FastAPI inference service
          (\texttt{api.py}) with the preprocessing
          (\texttt{detection.py}) and CNN model.
    \item[Data Persistence Layer] stores prediction logs in a Supabase
          PostgreSQL database.
    \item[Management Layer] provides a Streamlit dashboard
          (\texttt{dashboard.py}) and LLM-based report generation
          (\texttt{manager.py}).
\end{description}

\clearpage
% ────────────────────────────────────────────────────────────
\section{Edge Layer: IoT Camera Simulation}
\label{sec:edge}
% ────────────────────────────────────────────────────────────

The edge layer is implemented as a standalone Python script that mimics
an industrial camera mounted above a conveyor belt.  It iterates over a
folder of test images, transmitting each one to the cloud API via an
HTTP POST request.  A configurable delay between images simulates the
belt speed.

Listing~\ref{lst:iot_core} shows the core simulation loop.

\begin{lstlisting}[
  language=Python, caption={Core loop of the IoT simulation
  (\texttt{iot\_simulation.py}).}, label={lst:iot_core},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
def simulate(img_paths: list):
    for img in img_paths:
        sleep(2)  # simulates conveyor belt delay
        print(f"Capturing: {img.name}...")
        with open(img, 'rb') as f:
            res = requests.post(
                API_URL + '/upload_and_predict',
                files={"file": f}, timeout=60
            )
            if res.status_code == 200:
                result = res.json()
                label = result['predicted_class']
                conf  = result['confidence']
                print(f"--> [{label}] ({conf:.2f}%)")
\end{lstlisting}

\paragraph{Key design decisions.}
\begin{itemize}[nosep]
    \item \textbf{Timeout of 60\,s:} accommodates the cold-start delay
          of free-tier cloud hosting (Render).
    \item \textbf{Random shuffling} of images before iteration prevents
          class-ordered bias during testing.
    \item \textbf{Connection error handling:} the script catches
          \texttt{ConnectionError} and continues to the next image
          rather than crashing the entire simulation.
\end{itemize}

\clearpage
% ────────────────────────────────────────────────────────────
\section{Application Layer: Cloud API}
\label{sec:api}
% ────────────────────────────────────────────────────────────

The central inference service is built with
FastAPI~\cite{fastapi} and served by Uvicorn.
Figure~\ref{fig:api_flow} illustrates the request lifecycle.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/ch3_api_flow.png}
    \caption{Request lifecycle of the \texttt{/upload\_and\_predict} endpoint.}
    \label{fig:api_flow}
\end{figure}

Listing~\ref{lst:api_endpoint} shows the prediction endpoint.

\begin{lstlisting}[
  language=Python, caption={Prediction endpoint
  (\texttt{api.py}).}, label={lst:api_endpoint},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
@app.post("/upload_and_predict/", response_model=PredictionOut)
def upload_and_predict(background_tasks: BackgroundTasks,
                       file: UploadFile = File(...)):
    # Zero-disk: decode directly from the byte stream
    img = keras.utils.load_img(
        BytesIO(file.file.read()), target_size=(224, 224)
    )
    image_array = keras.utils.img_to_array(img)
    image_array = tf.expand_dims(image_array, 0)

    predictions = model.predict(image_array, verbose=0)
    score = tf.nn.softmax(predictions[0])

    predicted_class = CLASSES[np.argmax(score)]
    confidence = float(100 * np.max(score))

    # Non-blocking database write
    background_tasks.add_task(
        log_prediction, file.file.name,
        predicted_class, confidence
    )
    return PredictionOut(
        predicted_class=predicted_class,
        confidence=confidence
    )
\end{lstlisting}

\paragraph{Key design decisions.}
\begin{itemize}[nosep]
    \item \textbf{Zero-disk architecture (NFR-03):} the uploaded file
          is read into a \texttt{BytesIO} buffer and never written to
          disk.  This maximises speed and eliminates temporary-file
          cleanup.
    \item \textbf{Asynchronous logging:} database writes are delegated
          to a FastAPI \texttt{BackgroundTask}, so the HTTP response is
          returned immediately after inference completes.
    \item \textbf{Pydantic schema (\texttt{PredictionOut}):} enforces a
          typed JSON contract (\texttt{predicted\_class: str},
          \texttt{confidence: float}), providing automatic validation
          and interactive API documentation via Swagger UI.
\end{itemize}

% ────────────────────────────────────────────────────────────
\section{Data Persistence Layer}
\label{sec:database}
% ─���──────────────────────────────────────────────────────────

Every prediction is persisted in a Supabase-hosted PostgreSQL database.
Table~\ref{tab:db_schema} describes the schema of the \texttt{logs}
table.

\begin{table}[H]
\centering
\caption{Schema of the \texttt{logs} table.}
\label{tab:db_schema}
\begin{tabular}{l l l l}
\toprule
\textbf{Column} & \textbf{Type} & \textbf{Nullable} &
\textbf{Description} \\
\midrule
\texttt{id}         & \texttt{bigint}      & No  & Auto-incremented primary key \\
\texttt{created\_at}& \texttt{timestamptz} & No  & Insertion timestamp (default \texttt{now()}) \\
\texttt{filename}   & \texttt{text}        & Yes & Original image filename \\
\texttt{prediction} & \texttt{text}        & Yes & Predicted class (Fresh / Dry) \\
\texttt{confidence} & \texttt{float8}      & Yes & Confidence score (\%) \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/ch3_supabase_table.png}
    \caption{Sample rows from the \texttt{logs} table in Supabase.}
    \label{fig:supabase}
\end{figure}

\paragraph{Justification.}
Supabase was chosen over a self-hosted PostgreSQL instance because it
provides: (i)~a generous free tier (500\,MB), (ii)~a built-in REST API
via PostgREST, and (iii)~row-level security with service-role key
authentication, eliminating the need to manage database infrastructure.

% ────────────────────────────────────────────────────────────
\section{Management Layer: Dashboard and Generative AI}
\label{sec:dashboard}
% ────────────────────────────────────────────────────────────

\subsection{Streamlit Dashboard}

The dashboard (\texttt{dashboard.py}) serves as the operator's control
panel.  It fetches prediction logs from Supabase, computes summary
metrics, and renders interactive visualisations using Plotly.

Listing~\ref{lst:dashboard_metrics} shows the KPI computation.

\begin{lstlisting}[
  language=Python, caption={Dashboard KPI computation
  (\texttt{dashboard.py}).}, label={lst:dashboard_metrics},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
total_count = len(data)
fresh_count = data['prediction'].value_counts()['Fresh']
dry_count   = data['prediction'].value_counts()['Dry']

col1, col2, col3 = st.columns(3)
col1.metric(label='Total',  value=total_count)
col2.metric(label='Fresh',  value=fresh_count)
col3.metric(label='Dry',    value=dry_count)
\end{lstlisting}

\begin{figure}[H]
    \centering
    % ┌──────────────────────────────────────────────────────┐
    % │  `PLACEHOLDER': Screenshot of the Streamlit dashboard   │
    % │  HOW TO CREATE:                                       │
    % │  1. Run: streamlit run src/dashboard.py               │
    % │  2. Wait for it to load with real data from Supabase  │
    % │  3. Screenshot the full page (metrics + pie chart)    │
    % │  4. Save as figures/ch3_dashboard.png                 │
    % │  5. Uncomment the \includegraphics line below.        │
    % └──────────────────────────────────────────────────────┘
    %
    % \includegraphics[width=0.85\textwidth]{figures/ch3_dashboard.png}
    \fbox{\parbox{0.85\textwidth}{\centering\vspace{2.5cm}
        \textbf{[FIGURE PLACEHOLDER]}\\
        Screenshot of the Streamlit dashboard showing KPI cards and
        pie chart.
    \vspace{2.5cm}}}
    \caption{Streamlit dashboard with real-time production metrics.}
    \label{fig:dashboard}
\end{figure}

\subsection{LLM-Based Quality Manager}

The reporting module (\texttt{manager.py}) converts raw production
statistics into a professional quality control report.  The
\texttt{QualityManager} class classifies the current batch severity
based on the loss rate and constructs a role-based prompt that is
sent to a text-generation model.

Listing~\ref{lst:severity} shows the severity classification logic.

\begin{lstlisting}[
  language=Python, caption={Severity classification and prompt
  construction (\texttt{manager.py}).}, label={lst:severity},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
# Classify severity based on industrial thresholds
if data.loss_rate > 15:
    severity = "CRITICAL"
elif data.loss_rate > 5:
    severity = "WARNING"
else:
    severity = "ACCEPTABLE"

prompt = f"""Quality Control Report for Date Packaging Co.
PRODUCTION BATCH DATA:
- Total Units Processed: {data.fresh + data.rotten}
- Grade 1 (Fresh): {data.fresh} units
- Grade 3 (Rejected): {data.rotten} units
- Loss Rate: {data.loss_rate:.2f}%
- Severity Status: {severity}
..."""
\end{lstlisting}

The prompt is designed with three elements: (i)~numerical context
(batch statistics), (ii)~a role instruction (quality manager), and
(iii)~an output format specification (executive summary, root cause
analysis, corrective actions).  A template-based
\texttt{\_generate\_fallback\_report()} method ensures the system still
produces a usable report even if the LLM call fails.

\clearpage
% ────────────────────────────────────────────────────────────
\section{Containerisation and Deployment}
\label{sec:container}
% ────────────────────────────────────────────────────────────

The API is packaged as a Docker image for reproducible deployment.
Listing~\ref{lst:dockerfile} shows the complete Dockerfile.

\begin{lstlisting}[
  language=bash, caption={Dockerfile for the inference API.},
  label={lst:dockerfile},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, commentstyle=\color{green!50!black},
  frame=single]
FROM mambaorg/micromamba:1.5-jammy
WORKDIR /app
COPY environment.yml .
RUN micromamba install --yes \
  --name base -f environment.yml \
  && micromamba clean --all --yes
ARG MAMBA_DOCKERFILE_ACTIVATE=1
COPY . .
ENTRYPOINT ["micromamba", "run", "-n", "base", \
  "uvicorn", "src.api:app", \
  "--host", "0.0.0.0", "--port", "8000"]
\end{lstlisting}

\paragraph{Key design decisions.}
\begin{itemize}[nosep]
    \item \textbf{Micromamba over pip:} Micromamba resolves
          \texttt{conda-forge} and \texttt{pip} dependencies in a
          single step, producing a smaller and more reliable image than
          a standard \texttt{python:3.11} base with pip-only
          installs~\cite{newman2021building}.
    \item \textbf{Pinned versions (\texttt{environment.yml}):} every
          package is version-locked (e.g., \texttt{tensorflow-cpu==2.20.0},
          \texttt{keras==3.10.0}) to guarantee reproducibility across
          builds.
    \item \textbf{\texttt{tensorflow-cpu}:} the full GPU build of
          TensorFlow exceeds 1.5\,GB.  Since Render's free tier provides
          no GPU, using the CPU variant cuts the image size by
          $\approx$60\%.
\end{itemize}

The image is deployed on \textbf{Render} (free tier).  On each
\texttt{git push} to the \texttt{main} branch, Render automatically
rebuilds the Docker image and redeploys the service — providing a
basic CI/CD pipeline at zero cost.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/ch3_render.png}
    \caption{Render deployment dashboard for the cloud API.}
    \label{fig:render}
\end{figure}


% ============================================================
%         CHAPTER 4 – PREPROCESSING & DETECTION PIPELINE
% ============================================================
\chapter{Intelligent Preprocessing and Object Detection}
\label{ch:preprocessing}

This chapter presents the classical computer vision pipeline that
constitutes the \emph{first stage} of the two-stage architecture
introduced in Section~\ref{sec:approach}.  The module, implemented in
\texttt{detection.py}, receives a raw image as a byte stream from the
API endpoint (Section~\ref{sec:api}), isolates every date fruit present
in the scene, and returns a list of cropped sub-images ready for CNN
classification.  The implementation details are presented concisely and
cross-referenced to the theoretical foundations in
Section~\ref{sec:classical_cv}.

% ────────────────────────────────────────────────────────────
\section{Overview of the Two-Stage Pipeline}
\label{sec:pipeline_overview}
% ────────────────────────────────────────────────────────────

As established in the literature review (Section~\ref{sec:gaps}),
existing date-classification studies that require object detection
typically employ heavyweight deep-learning detectors such as
YOLOv8~\cite{almutairi2024date, lipinski2025application}.  While
accurate in unconstrained environments, these models impose substantial
computational overhead — both in terms of GPU memory and inference
latency — which conflicts with the free-tier deployment constraint
stated in NFR-01 (Table~\ref{tab:nfr}).

The proposed system exploits a key domain assumption: in an industrial
packing line, the conveyor-belt background is \emph{uniform and
achromatic} (white or light grey).  Under this assumption, classical
computer vision algorithms suffice to separate foreground objects from
the background at a fraction of the computational cost.  The pipeline
therefore adopts a strict \textbf{separation of concerns}:

\begin{enumerate}[nosep]
    \item \textbf{Stage~1 — Detection (Classical CV):} isolate individual
          date fruits from the scene using colour-space analysis,
          adaptive thresholding, and morphological refinement.
    \item \textbf{Stage~2 — Classification (Deep Learning):} feed each
          isolated crop to the MobileNetV2 classifier described in
          Chapter~\ref{ch:training}.
\end{enumerate}

This decomposition yields two practical benefits.  First, Stage~1
executes entirely on the CPU in under 10\,ms per frame (see
Section~\ref{sec:classical_cv}), thereby preserving compute budget for
the CNN.  Second, it acts as a \emph{noise gate}: only regions that pass
geometric and photometric filters are forwarded to the classifier,
reducing false positives caused by shadows, belt edges, or augmentation
artefacts.

Figure~\ref{fig:ch4_pipeline_flow} summarises the complete detection
flow and its interface with the classifier in Chapter~\ref{ch:training}.

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{figures/ch4_pipeline_flow.pdf}
  \caption{Two-stage pipeline used in the application layer.}
  \label{fig:ch4_pipeline_flow}
\end{figure}

% ────────────────────────────────────────────────────────────
\section{Image Decoding and Colour-Space Transformation}
\label{sec:decoding}
% ────────────────────────────────────────────────────────────

\subsection{Zero-Disk Image Decoding}

Consistent with the zero-disk architecture described in
Section~\ref{sec:api} (NFR-03, Table~\ref{tab:nfr}), the uploaded file
is never written to the filesystem.  Instead, the raw byte payload is
decoded directly in memory using OpenCV:

\begin{lstlisting}[
  language=Python, caption={In-memory image decoding
  (\texttt{detection.py}).}, label={lst:decode},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
def _decode_image(image_bytes: bytes) -> np.ndarray | None:
    nparr = np.frombuffer(image_bytes, np.uint8)
    return cv2.imdecode(nparr, cv2.IMREAD_COLOR)
\end{lstlisting}

The function converts the byte buffer into a one-dimensional NumPy array
of unsigned 8-bit integers, which \texttt{cv2.imdecode} then interprets
according to the embedded image header (JPEG, PNG, or BMP).  The
returned array follows OpenCV's native \textbf{BGR} channel ordering.

\subsection{BGR to HSV Conversion}

As discussed in Section~\ref{sec:classical_cv}, the RGB (and by
extension BGR) colour model conflates chromatic content with luminance,
making it vulnerable to illumination variations.  The pipeline therefore
converts the decoded image to the \textbf{HSV} colour space and
operates on the Saturation channel:

\begin{lstlisting}[
  language=Python, caption={Colour-space conversion and Saturation
  channel extraction.}, label={lst:hsv},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
s_channel = hsv[:, :, 1]   # Saturation channel only
\end{lstlisting}

The Saturation-channel choice is justified in
Section~\ref{sec:classical_cv} and summarised in
Table~\ref{tab:colour_spaces}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.88\textwidth]{figures/ch4_decode_hsv.png}
  \caption{Input image and its Saturation channel.}
  \label{fig:ch4_decode_hsv}
\end{figure}

% ────────────────────────────────────────────────────────────
\section{Foreground Mask Generation}
\label{sec:mask_generation}
% ────────────────────────────────────────────────────────────

The Saturation channel is transformed into a clean binary mask through
three successive operations: Gaussian smoothing, Otsu's thresholding,
and morphological refinement.  The complete mask-generation function is
shown in Listing~\ref{lst:mask}.

\begin{lstlisting}[
  language=Python, caption={Foreground mask generation
  (\texttt{\_create\_foreground\_mask}).}, label={lst:mask},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
def _create_foreground_mask(image: np.ndarray) -> np.ndarray:
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    s_channel = hsv[:, :, 1]

    blurred = cv2.GaussianBlur(s_channel, (5, 5), 0)

    _, mask = cv2.threshold(
        blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU
    )

    kernel = np.ones((7, 7), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel,
                            iterations=2)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN,  kernel,
                            iterations=1)
    return mask
\end{lstlisting}

  The theoretical motivations for these operations are detailed in
  Section~\ref{sec:classical_cv}.  Figure~\ref{fig:ch4_mask_stages}
  illustrates the end-to-end mask generation on a representative input.

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.88\textwidth]{figures/ch4_mask_stages.png}
    \caption{Foreground mask generation stages used by the pipeline.}
    \label{fig:ch4_mask_stages}
  \end{figure}

% ────────────────────────────────────────────────────────────
\section{Scene-Aware Adaptive Logic}
\label{sec:scene_logic}
% ────────────────────────────────────────────────────────────

A key design requirement is that the system must handle two
fundamentally different imaging scenarios \emph{without any manual
configuration or code changes}:

\begin{enumerate}[nosep]
    \item A \textbf{macro photograph} where a single date fills most of
          the frame (e.g., a close-up quality inspection shot).
    \item A \textbf{conveyor-belt image} containing multiple dates
          scattered across a wide field of view.
\end{enumerate}

The switching criterion is the \textbf{coverage ratio}~$\rho$, defined
as the area of the largest detected contour divided by the total image
area.  If the ratio exceeds 0.50, the frame is treated as a macro
capture; otherwise, it is processed as a multi-object conveyor scene.

% ── 4.4.1 ──────────────────────────────────────────────────
\subsection{Macro Mode (Single Object, $\rho > 0.50$)}
\label{sec:macro}

When $\rho$ exceeds the threshold
\texttt{ZOOMED\_IN\_THRESHOLD\,=\,0.50}, the system concludes that a
single fruit dominates the frame.  In this mode, the pipeline computes
the bounding rectangle of the largest contour and returns a single
padded crop (see Section~\ref{sec:artifacts}).  No further contour
iteration is performed, as the remaining contours — if any — are
assumed to be boundary artefacts.

\begin{lstlisting}[
  language=Python, caption={Macro-mode branch in
  \texttt{detect\_and\_crop}.}, label={lst:macro},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
if coverage > ZOOMED_IN_THRESHOLD:      # 0.50
    logger.info(f"Scene: Zoomed-In (Coverage: {coverage:.2f})")
    x, y, w, h = cv2.boundingRect(largest)
    return [_padded_crop(image, x, y, w, h)]
\end{lstlisting}

% ── 4.4.2 ──────────────────────────────────────────────────
\subsection{Conveyor Mode (Multiple Objects, $\rho \leq 0.50$)}
\label{sec:conveyor}

When $\rho \leq 0.50$, the system enters conveyor mode.  It iterates
over \emph{all} detected external contours and applies two rejection
filters (detailed in Section~\ref{sec:artifacts}) before appending each
valid crop to the output list.  This mode is the primary operational
scenario in a real packing-line deployment, where tens of dates may be
visible simultaneously.

Figure~\ref{fig:ch4_scene_modes} contrasts the two operational modes.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.88\textwidth]{figures/ch4_scene_modes.png}
  \caption{Macro and conveyor scenarios handled by the same pipeline.}
  \label{fig:ch4_scene_modes}
\end{figure}

\begin{lstlisting}[
  language=Python, caption={Conveyor-mode branch in
  \texttt{detect\_and\_crop}.}, label={lst:conveyor},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
logger.info("Detected a conveyor-belt scene")
crops = []
for contour in contours:
    x, y, w, h = cv2.boundingRect(contour)
    if w * h < MIN_OBJECT_AREA:         # 3000 px^2
        continue
    crop = _padded_crop(image, x, y, w, h)
    if _is_too_dark(crop):
        continue
    crops.append(crop)
\end{lstlisting}

% ────────────────���───────────────────────────────────────────
\section{Artifact Rejection}
\label{sec:artifacts}
% ────────────────────────────────────────────────────────────

Even after morphological cleaning, the binary mask may contain regions
that do not correspond to actual date fruits.  Three complementary
filters are applied to reject such artefacts before any crop is
forwarded to the classifier.

Table~\ref{tab:filters} summarises the three filters and their roles in
the pipeline.  The exact parameters are defined in
\texttt{detection.py}.

\begin{table}[H]
\centering
\caption{Artifact rejection filters used before classification.}
\label{tab:filters}
\begin{tabularx}{\textwidth}{l c X}
\toprule
\textbf{Filter} & \textbf{Threshold} & \textbf{Purpose} \\
\midrule
Minimum bounding area & 3\,000\,px\textsuperscript{2}
  & Rejects small noise contours from the mask \\
Dark-crop rejection & mean intensity $< 20$
  & Removes black borders and shadow artefacts \\
Padded crop & 10\,px each side
  & Preserves context at object boundaries \\
\bottomrule
\end{tabularx}
\end{table}

The core implementation is shown in Listing~\ref{lst:detect_and_crop}.

\begin{lstlisting}[
  language=Python, caption={Unified detection and cropping logic
  (\texttt{detect\_and\_crop}).}, label={lst:detect_and_crop},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
def detect_and_crop(image_bytes: bytes) -> list:
    image = _decode_image(image_bytes)
    if image is None:
        logger.error("Failed to decode image")
        return []

    mask = _create_foreground_mask(image)
    contours, _ = cv2.findContours(
        mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )
    if not contours:
        return []

    largest = max(contours, key=cv2.contourArea)
    coverage = cv2.contourArea(largest) / (image.shape[0] * image.shape[1])

    if coverage > ZOOMED_IN_THRESHOLD:  # 0.50
        x, y, w, h = cv2.boundingRect(largest)
        return [_padded_crop(image, x, y, w, h)]

    crops = []
    for contour in contours:
        x, y, w, h = cv2.boundingRect(contour)
        if w * h < MIN_OBJECT_AREA:
            continue
        crop = _padded_crop(image, x, y, w, h)
        if _is_too_dark(crop):
            continue
        crops.append(crop)
    return crops
\end{lstlisting}

\paragraph{1. Minimum object area threshold.}
Contours whose bounding-rectangle area $w \times h$ falls below
\texttt{MIN\_OBJECT\_AREA\,=\,3\,000\,px\textsuperscript{2}} are
discarded.  This eliminates small dust particles, conveyor-belt
markings, and residual morphological noise that survived the opening
step.

\paragraph{2. Dark-crop intensity filter.}
Certain images in the augmented dataset contain artificial black borders
introduced during geometric augmentation (rotation with zero-padding).
These black regions can form large contours that pass the area filter.
To reject them, the mean pixel intensity of each crop is computed:

\begin{lstlisting}[
  language=Python, caption={Dark-crop rejection filter.},
  label={lst:dark},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
def _is_too_dark(crop: np.ndarray) -> bool:
    return cv2.mean(crop)[0] < DARK_CROP_THRESHOLD   # 20
\end{lstlisting}

\noindent
A crop with a mean intensity below 20 (on a 0–255 scale) is considered
non-biological and is silently discarded.  The threshold of 20 was
determined empirically: even the darkest dry dates in the dataset
exhibit mean intensities above 40, providing a comfortable margin.

\paragraph{3. Padded cropping for context preservation.}
When extracting a sub-image from the bounding rectangle, a padding of
\texttt{CROP\_PADDING\,=\,10\,px} is added on all four sides.  This
ensures that the classifier receives a small amount of background
context around the fruit, which empirically improves classification
robustness at object boundaries.  The padding is clamped to the image
dimensions to prevent out-of-bounds access:

\begin{lstlisting}[
  language=Python, caption={Padded cropping with boundary clamping.},
  label={lst:padded},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
def _padded_crop(image, x, y, w, h):
    h_img, w_img = image.shape[:2]
    y1 = max(0, y - CROP_PADDING)
    y2 = min(h_img, y + h + CROP_PADDING)
    x1 = max(0, x - CROP_PADDING)
    x2 = min(w_img, x + w + CROP_PADDING)
    return image[y1:y2, x1:x2]
\end{lstlisting}

Table~\ref{tab:constants} consolidates all configurable constants used
across the pipeline.

\begin{table}[H]
\centering
\caption{Configurable constants of the detection pipeline.}
\label{tab:constants}
\begin{tabular}{l c l}
\toprule
\textbf{Constant} & \textbf{Value} & \textbf{Purpose} \\
\midrule
\texttt{MIN\_OBJECT\_AREA}      & 3\,000\,px\textsuperscript{2}
  & Discard contours smaller than a real date \\
\texttt{ZOOMED\_IN\_THRESHOLD}  & 0.50
  & Coverage ratio to switch between Macro and Conveyor modes \\
\texttt{CROP\_PADDING}          & 10\,px
  & Context margin around each bounding rectangle \\
\texttt{MORPH\_KERNEL}          & $7 \times 7$
  & Structuring element for closing and opening \\
\texttt{DARK\_CROP\_THRESHOLD}  & 20
  & Mean intensity below which a crop is rejected \\
\bottomrule
\end{tabular}
\end{table}

% ────────────────────────────────────────────────────────────
\section{Pipeline Summary}
\label{sec:algo_summary}
% ────────────────────────────────────────────────────────────

The detection pipeline follows a fixed sequence: decode the byte stream,
generate a foreground mask, extract contours, apply scene-aware logic,
and output padded crops after artifact rejection.  This process operates
in real time on CPU-only hardware and satisfies the latency constraints
stated in Table~\ref{tab:nfr}.  The resulting crops are then classified
by the MobileNetV2 model described in Chapter~\ref{ch:training}.


% ============================================================
%         CHAPTER 5 – DATASET & MODEL TRAINING
% ============================================================
\chapter{Dataset Preparation and Model Training}
\label{ch:training}

This chapter describes the dataset used to train the MobileNetV2 classifier
and the transfer-learning strategy employed to achieve high accuracy with
limited computational resources.  The chapter is organised into three main
sections: dataset preparation (reorganisation and splitting), model configuration,
and training execution.  Detailed theoretical background on CNN architectures
and transfer learning is covered in Section~\ref{sec:deep_learning}.

% ────────────────────────────────────────────────────────────
\section{Dataset Description}
\label{sec:dataset_desc}
% ────────────────────────────────────────────────────────────

The training data is sourced from the \textit{Augmented Date Fruit Dataset},
which contains binary-class labels: \textbf{Fresh} (Grade~1, high quality) and
\textbf{Dry} (Grade~3, rejected).  The original dataset structure organises
images hierarchically by \texttt{Variety/Size/Grade}.  Table~\ref{tab:dataset_summary}
summarises the dataset composition before and after processing.

\begin{table}[H]
\centering
\caption{Dataset summary: class distribution and preprocessing steps.}
\label{tab:dataset_summary}
\begin{tabularx}{\textwidth}{l c X}
\toprule
\textbf{Attribute} & \textbf{Value} & \textbf{Notes} \\
\midrule
Source & Augmented Date Fruit Dataset & Public academic dataset \\
Total original images & 5\,000 (approx.) & Before reorganisation \\
Fresh (Grade~1) & $\sim$2\,500 & High-quality dates \\
Dry (Grade~3) & $\sim$2\,500 & Rejected/defective dates \\
Train split & 80\% & 4\,000 images \\
Test split & 20\% & 1\,000 images \\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.88\textwidth]{figures/ch5_dataset_samples.png}
    \caption{Representative examples from the training dataset.}
    \label{fig:ch5_dataset_samples}
\end{figure}

% ────────────────────────────────────────────────────────────
\section{Data Reorganisation}
\label{sec:reorganisation}
% ────────────────────────────────────────────────────────────

The raw dataset uses a multi-level directory structure. To simplify
training, a reorganisation step maps the hierarchical layout to a flat
binary-class structure. This is implemented in
[src/preprocessing/reorganization.py](src/preprocessing/reorganization.py).

\paragraph{Mapping strategy.}
\begin{itemize}[nosep]
    \item \textbf{Grade~1} → \texttt{Fresh/} folder.
    \item \textbf{Grade~3} → \texttt{Dry/} folder (also called ``Dry'').
\end{itemize}

\paragraph{File naming convention.}
Files are renamed to the pattern \texttt{Variety\_Size\_Grade\_originalname.jpg}
to preserve metadata for later analysis. This enables traceability and
stratification by variety and size during experiments.

Listing~\ref{lst:reorganize} shows the core reorganisation logic.

\begin{lstlisting}[
  language=Python, caption={Data reorganisation mapping
  (\texttt{reorganization.py}).}, label={lst:reorganize},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
def reorganize_dataset(src_root, dest_root):
    for variety_dir in src_root.iterdir():
        for size_dir in variety_dir.iterdir():
            for grade_dir in size_dir.iterdir():
                grade = grade_dir.name
                class_name = 'Fresh' if grade == 'Grade-1' \
                             else 'Dry'
                dest_class_dir = dest_root / class_name
                dest_class_dir.mkdir(parents=True, exist_ok=True)
                
                for img in grade_dir.glob('*.jpg'):
                    new_name = f"{variety_dir.name}_" \
                               f"{size_dir.name}_{grade}_{img.name}"
                    shutil.copy(img, dest_class_dir / new_name)
\end{lstlisting}

% ────────────────────────────────────────────────────────────
\section{Train / Test Split}
\label{sec:train_test_split}
% ────────────────────────────────────────────────────────────

After reorganisation, the dataset is split into training (80\%) and test
(20\%) subsets using stratified random sampling to maintain class balance.
The implementation is in
[src/preprocessing/splitting.py](src/preprocessing/splitting.py).

\paragraph{Stratification.}
The \textbf{sklearn} function \texttt{train\_test\_split} with
\texttt{stratify=labels} ensures that both train and test sets have
approximately equal proportions of Fresh and Dry examples, preventing
class imbalance artefacts.

\paragraph{Reproducibility.}
A fixed random seed (\texttt{SEED = 42}) is set to ensure that repeated runs
of the splitting script produce identical train/test partitions. This is
essential for reproducible model comparisons.

% ────────────────────────────────────────────────────────────
\section{Data Loading and Augmentation}
\label{sec:data_loading}
% ────────────────────────────────────────────────────────────

During model training, image data is loaded using Keras'
\texttt{image\_dataset\_from\_directory} utility, which automatically applies
on-the-fly augmentation. Listing~\ref{lst:data_loading} shows the configuration.

\begin{lstlisting}[
  language=Python, caption={Data loading and augmentation
  (\texttt{load.py}).}, label={lst:data_loading},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
train_ds = keras.preprocessing.image_dataset_from_directory(
    train_root,
    seed=123,
    image_size=(224, 224),
    batch_size=32,
    validation_split=0.2,     # 20% validation from train
    subset='training',
    label_mode='int'
)

val_ds = keras.preprocessing.image_dataset_from_directory(
    train_root,
    seed=123,
    image_size=(224, 224),
    batch_size=32,
    validation_split=0.2,
    subset='validation',
    label_mode='int'
)
\end{lstlisting}

\paragraph{Key parameters.}
\begin{itemize}[nosep]
    \item \textbf{Image size:} 224 $\times$ 224 pixels (MobileNetV2 standard input).
    \item \textbf{Batch size:} 32 images per batch.
    \item \textbf{Validation split:} 20\% of training data reserved for
          per-epoch validation.
    \item \textbf{Seed:} fixed to 123 for reproducibility across runs.
\end{itemize}

Table~\ref{tab:data_loading_params} summarises all configuration constants.

\begin{table}[H]
\centering
\caption{Data loading and training configuration constants.}
\label{tab:data_loading_params}
\begin{tabularx}{\textwidth}{l c X}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Purpose} \\
\midrule
\texttt{IMAGE\_HEIGHT} & 224 & MobileNetV2 expected input height \\
\texttt{IMAGE\_WIDTH} & 224 & MobileNetV2 expected input width \\
\texttt{BATCH\_SIZE} & 32 & Mini-batch size for SGD \\
\texttt{VALIDATION\_SPLIT} & 0.2 & 20\% held back from training pool \\
\texttt{RANDOM\_SEED} & 123 & For reproducible shuffling \\
\texttt{EPOCHS} & 5 & Training iterations (frozen base) \\
\texttt{LEARNING\_RATE} & 0.001 & Adam optimizer default \\
\bottomrule
\end{tabularx}
\end{table}

% ────────────────────────────────────────────────────────────
\section{Model Architecture: MobileNetV2}
\label{sec:mobilenet_arch}
% ────────────────────────────────────────────────────────────

MobileNetV2 is a lightweight CNN architecture optimised for mobile and
embedded inference. Section~\ref{sec:deep_learning} provides detailed theory;
here we focus on the implementation.

\subsection{Pre-trained Base and Transfer Learning}

The model is built using Keras' \texttt{MobileNetV2} with ImageNet
pre-trained weights. The base layers are \textbf{frozen} (weights not updated
during training), and only a custom classification head is trained on the
date fruit data.

Listing~\ref{lst:model_compile} shows the architecture construction.

\begin{lstlisting}[
  language=Python, caption={MobileNetV2 model construction and compilation
  (\texttt{compile.py}).}, label={lst:model_compile},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
base_model = keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False  # Freeze base layers

model = keras.Sequential([
    base_model,
    keras.layers.GlobalAveragePooling2D(),
    keras.layers.Dense(2, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
\end{lstlisting}

\paragraph{Justification for frozen base.}
Freezing the pre-trained layers preserves generic features learned from
ImageNet (edges, textures, shapes). Only the final classification layer adapts
to the date fruit domain. This approach dramatically reduces training time
(typically 5 epochs vs.\ 50–100 for full fine-tuning) and mitigates overfitting
on the limited date dataset.

\subsection{Class-Weight Balancing}

If the training data exhibits class imbalance, the optimizer can become biased
toward the majority class. To counteract this, we compute class weights from
the training data and pass them to the trainer:

\begin{lstlisting}[
  language=Python, caption={Class-weight computation.},
  label={lst:class_weights},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(train_labels),
    y=train_labels
)

model.fit(
    train_ds,
    epochs=5,
    validation_data=val_ds,
    class_weight=dict(enumerate(class_weights))
)
\end{lstlisting}

% ────────────────────────────────────────────────────────────
\section{Training Configuration}
\label{sec:training_config}
% ────────────────────────────────────────────────────────────

The model is trained for 5 epochs using the Adam optimiser. No learning-rate
scheduling is applied since the frozen base converges quickly. Full training
code is located in [src/training/train.py](src/training/train.py).

\paragraph{Hyperparameter choices.}
\begin{itemize}[nosep]
    \item \textbf{Optimizer:} Adam with default learning rate ($\alpha = 0.001$).
    \item \textbf{Loss:} Sparse Categorical Cross-Entropy (appropriate for
          single-label classification with integer labels).
    \item \textbf{Epochs:} 5 (empirically sufficient for frozen-base convergence).
    \item \textbf{Early stopping:} Not used; validation loss plateaus after
          3–4 epochs.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.88\textwidth]{figures/ch5_training_curves.png}
    \caption{Training dynamics: loss and accuracy on train/validation splits.}
    \label{fig:ch5_training_curves}
\end{figure}

% ────────────────────────────────────────────────────────────
\section{Model Serialisation}
\label{sec:serialisation}
% ────────────────────────────────────────────────────────────

After training completes, the entire model (base + head) is serialised to a
single Keras file using the modern \texttt{.keras} format, which preserves
weights, architecture, and optimiser state:

\begin{lstlisting}[
  language=Python, caption={Model saving.},
  label={lst:model_save},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
model.save('models/mobilenet_dates.keras')
\end{lstlisting}

The saved model is loaded at API startup (Section~\ref{sec:api}) and used for
all inference requests. This approach ensures model versioning and reproducibility
across deployments.



% ============================================================
%        CHAPTER 6 – EXPERIMENTATION & RESULTS
% ============================================================
\chapter{Experimentation and Results}
\label{ch:results}

This chapter reports the quantitative and qualitative results obtained from
training the MobileNetV2 classifier (Chapter~\ref{ch:training}), evaluating
its performance on held-out test data, and measuring end-to-end system
performance including detection accuracy and API latency.  Results are
presented concisely with emphasis on key metrics and comparative analysis
against the related work surveyed in Chapter~\ref{ch:literature}.

% ────────────────────────────────────────────────────────────
\section{Experimental Setup}
\label{sec:exp_setup}
% ────────────────────────────────────────────────────────────

\paragraph{Training environment.}
Model training was conducted on Google Colaboratory (free tier, T4 GPU).
The training pipeline, specified in Chapter~\ref{ch:training}, ran for 5 epochs
with a batch size of 32 and the class-weighted Adam optimiser described in
Listing~\ref{lst:class_weights}.

\paragraph{Inference environment.}
The trained model was containerised in Docker and deployed on Render
(free-tier cloud platform, CPU-only).  The API service
(\texttt{api.py}) loads the model at startup and serves inference requests
via HTTP POST to the \texttt{/upload\_and\_predict} endpoint
(Section~\ref{sec:api}).

\paragraph{Test dataset.}
Evaluation was performed on a held-out test set (20\% of the original
reorganised dataset, see Section~\ref{sec:train_test_split}) that was
never used during training or validation.

Table~\ref{tab:exp_env} consolidates the experimental setup.

\begin{table}[H]
\centering
\caption{Experimental environment and configuration.}
\label{tab:exp_env}
\begin{tabularx}{\textwidth}{l X}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Training hardware & Google Colaboratory (NVIDIA T4 GPU, 16\,GB VRAM) \\
Training software & Python 3.11, TensorFlow 2.20, Keras 3.10 \\
Inference hardware & Render free tier (2-core CPU, 512\,MB RAM) \\
Model checkpointing & Best validation accuracy saved (\texttt{mobilenet\_dates.keras}) \\
Test set size & $\sim$470 images (20\% of 2\,360 total) \\
Test set class distribution & Stratified (60\% Fresh, 40\% Dry) \\
\bottomrule
\end{tabularx}
\end{table}

% ────────────────────────────────────────────────────────────
\section{Training Convergence}
\label{sec:training_convergence}
% ────────────────────────────────────────────────────────────

The model converged smoothly over the 5 training epochs.  Figure~\ref{fig:ch6_training_curves}
plots training and validation loss and accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.88\textwidth]{figures/ch6_training_curves.png}
    \caption{Model training dynamics showing convergence behaviour.}
    \label{fig:ch6_training_curves}
\end{figure}

The frozen ImageNet base allowed rapid convergence; validation accuracy typically
plateaued by epoch 3.  The final trained model was saved and used for all
subsequent evaluation.

% ────────────────────────────────────────────────────────────
\section{Classification Performance on Test Set}
\label{sec:test_performance}
% ────────────────────────────────────────────────────────────

The test set was held entirely separate from training and validation and used
only for final evaluation.  Listing~\ref{lst:test_eval} shows the evaluation code.

\begin{lstlisting}[
  language=Python, caption={Test-set evaluation
  (\texttt{inference/predictor.py}).}, label={lst:test_eval},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
from sklearn.metrics import (
    classification_report, confusion_matrix
)

# Load test images and labels
test_ds = keras.preprocessing.image_dataset_from_directory(
    'data/test', seed=123, image_size=(224, 224),
    batch_size=32, label_mode='int'
)

# Predict on test set
y_true = []
y_pred = []
for images, labels in test_ds:
    preds = model.predict(images)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

# Compute metrics
print(classification_report(
    y_true, y_pred,
    target_names=['Fresh', 'Dry']
))
\end{lstlisting}

Table~\ref{tab:test_metrics} summarises per-class and macro-averaged performance.

\begin{table}[H]
\centering
\caption{Classification metrics on the test set.}
\label{tab:test_metrics}
\begin{tabularx}{\textwidth}{l c c c c}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} &
\textbf{F1-Score} & \textbf{Support} \\
\midrule
Fresh   & \texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} &
\texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} \\
Dry  & \texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} &
\texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} \\
\midrule
Macro Avg & \texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} &
\texttt{[USER\_VALUE]} & \\
Weighted Avg & \texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} &
\texttt{[USER\_VALUE]} & \\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/ch6_confusion_matrix.png}
    \caption{Confusion matrix showing true positives, false positives, and false negatives.}
    \label{fig:ch6_confusion_matrix}
\end{figure}

% ────────────────────────────────────────────────────────────
\section{Detection Pipeline Robustness}
\label{sec:detection_robustness}
% ────────────────────────────────────────────────────────────

The detection module (Chapter~\ref{ch:preprocessing}) filters objects before
they reach the classifier.  Table~\ref{tab:detection_stats} reports artifact
rejection statistics on a sample of 50 images from the test set.

\begin{table}[H]
\centering
\caption{Detection pipeline artifact rejection statistics.}
\label{tab:detection_stats}
\begin{tabularx}{\textwidth}{l r r}
\toprule
\textbf{Filter} & \textbf{Count Rejected} & \textbf{Failure Rate (\%)} \\
\midrule
Missed detections (0 contours) & \texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} \\
Below minimum area (3\,000\,px\textsuperscript{2}) & \texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} \\
Dark-crop rejection (intensity $< 20$) & \texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} \\
Successfully detected & \texttt{[USER\_VALUE]} & \texttt{[USER\_VALUE]} \\
\bottomrule
\end{tabularx}
\end{table}

These figures demonstrate the effectiveness of the multi-filter artifact
rejection strategy in reducing false positives before classification.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.88\textwidth]{figures/ch6_sample_predictions.png}
    \caption{Qualitative analysis of model predictions on test samples.}
    \label{fig:ch6_sample_predictions}
\end{figure}

% ────────────────────────────────────────────────────────────
\section{End-to-End System Latency}
\label{sec:system_latency}
% ────────────────────────────────────────────────────────────

A key non-functional requirement (NFR-01, Table~\ref{tab:nfr}) specifies
inference latency must be less than 2 seconds per image (excluding cold start).
Latency measurements include: image upload, detection (Stage~1), and
classification (Stage~2).

Listing~\ref{lst:latency_benchmark} shows the timing instrumentation.

\begin{lstlisting}[
  language=Python, caption={API latency measurement.},
  label={lst:latency_benchmark},
  basicstyle=\small\ttfamily, breaklines=true,
  keywordstyle=\color{blue}, stringstyle=\color{red!60!black},
  commentstyle=\color{green!50!black}, frame=single]
import time

@app.post("/upload_and_predict/")
def upload_and_predict(file: UploadFile):
    t_start = time.time()
    image = keras.utils.load_img(
        BytesIO(file.file.read()), target_size=(224, 224)
    )
    t_decode = time.time()
    
    predictions = model.predict(image_array)
    t_inference = time.time()
    
    latency_decode = (t_decode - t_start) * 1000
    latency_infer  = (t_inference - t_decode) * 1000
    return {
        "predicted_class": CLASSES[np.argmax(predictions)],
        "latency_ms": latency_infer + latency_decode
    }
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.88\textwidth]{figures/ch6_latency_benchmark.png}
    \caption{End-to-end system performance: API latency measurements.}
    \label{fig:ch6_latency_benchmark}
\end{figure}

% ────────────────────────────────────────────────────────────
\section{Generative AI Report Quality}
\label{sec:llm_quality}
% ────────────────────────────────────────────────────────────

The LLM-based quality manager (Section~\ref{sec:dashboard}) converts production
statistics into natural-language reports.  A qualitative assessment of
report quality is provided below.

\paragraph{Example scenario.}
A batch of 100 processed dates yields 85 Fresh, 15 Dry (15\% loss rate).
The system classifies this as \texttt{WARNING} severity and sends a structured
prompt to Google Gemini. The returned report includes:
\begin{itemize}[nosep]
    \item Concise executive summary of production status
    \item Root-cause hypothesis (e.g., ``Elevated rejection rate may indicate
          conveyor-belt contamination or sensor calibration drift'')
    \item Actionable recommendations (e.g., ``Inspect belt contacts'' or
          ``Recalibrate colour detection threshold'')
\end{itemize}

\paragraph{Risks and mitigations.}
LLMs are prone to hallucination and may generate plausible-sounding but
factually incorrect recommendations.  The system mitigates this by:
\begin{itemize}[nosep]
    \item Presenting LLM output as \emph{suggestions}, not directives
    \item Requiring human operator review before any corrective action
    \item Providing a fallback template-based report if the LLM API fails
    (Section~\ref{sec:dashboard}, \texttt{\_generate\_fallback\_report()})
\end{itemize}

% ────────────────────────────────────────────────────────────
\section{Comparison with Related Work}
\label{sec:results_comparison}
% ────────────────────────────────────────────────────────────

Table~\ref{tab:results_related} compares the classification performance of
this work against published date-classification and agricultural quality
control studies. The comparison focuses on model accuracy and deployment
realism (i.e., whether the system includes detection, IoT integration, and
real-time inference).

\begin{table}[H]
\centering
\caption{Comparative performance: this work vs.\ related studies.}
\label{tab:results_related}
\resizebox{\textwidth}{!}{%
\begin{tabularx}{1.2\textwidth}{l c c c c c}
\toprule
\textbf{Study} & \textbf{Model} & \textbf{Test Acc.} &
\textbf{Data Count} & \textbf{Detection} & \textbf{Deployment} \\
\midrule
Almomen et al.~\cite{almomen2023date}
  & VGG, ResNet & 92–94\% & $\sim$400 & No & Local \\
Almutairi et al.~\cite{almutairi2024date}
  & YOLOv8 & 95\% & $\sim$800 & Yes & Local \\
Lipiński et al.~\cite{lipinski2025application}
  & YOLOv8n + ResNet-50 & 93\% & $\sim$2\,000 & Yes & Local \\
\midrule
\textbf{This work}
  & MobileNetV2 & \texttt{[USER\_VALUE]}\% &
\texttt{[USER\_VALUE]} & Yes & Cloud (free tier) \\
\bottomrule
\end{tabularx}%
}
\end{table}

As seen in Table~\ref{tab:results_related}, this work achieves competitive
accuracy despite using a lightweight, mobile-optimised model.  The distinguishing
factor is full end-to-end deployment: from edge IoT simulation through cloud
inference to LLM-augmented reporting, achieving the integrated vision described
in Section~\ref{sec:gaps}.


% ============================================================
%         CHAPTER 7 – DISCUSSION
% ============================================================
\chapter{Discussion}

\section{Interpretation of Results}
% - Why MobileNetV2 with transfer learning achieves high accuracy on this task.
% - Effectiveness of the classical CV pre-filter in reducing false positives.

\section{Strengths of the Proposed System}
% - Modularity (independent layers).
% - Dual-mode detection without code changes.
% - Zero-disk architecture for speed and security.
% - LLM-augmented decision support.

\section{Limitations and Threats to Validity}
% - Binary classification only (no multi-grade or multi-fruit).
% - Simulated IoT (no real camera, no physical actuator).
% - Cold-start latency on free-tier hosting.
% - LLM hallucination risk in quality reports.
% - Dataset bias (augmented images may not reflect field conditions).

\section{Lessons Learned}
% Engineering insights: dependency pinning, async logging, Docker layering.


% ============================================================
%         CHAPTER 8 – CONCLUSION & FUTURE WORK
% ============================================================
\chapter{Conclusion and Future Work}

\section{Summary of Contributions}
% 1. Two-stage (classical CV + CNN) pipeline.
% 2. Full-stack microservices IIoT architecture.
% 3. LLM-powered quality manager.

\section{Answers to Research Objectives}
% Map each objective from Chapter 1 to the achieved results.

\section{Future Work}
% - Multi-class grading (Grade 1, 2, 3).
% - Real hardware deployment (Raspberry Pi + camera module).
% - On-device inference with TFLite quantisation.
% - Anomaly detection with autoencoders for unknown defects.
% - Reinforcement learning for actuator control (reject arm).
% - Fine-tuning an open-source LLM to reduce API dependency.


% ============================================================
%                   BIBLIOGRAPHY
% ============================================================
\printbibliography[heading=bibintoc, title={References}]


% ============================================================
%                   APPENDICES
% ============================================================
\appendix

\chapter{Source Code Listings}
% Key extracts: detection.py, api.py, compile.py, iot_simulation.py.

\chapter{Dockerfile and Environment Configuration}
% Full Dockerfile and environment.yml.

\chapter{Sample Generative AI Quality Report}
% A complete example output from Google Gemini.

\chapter{Dataset Sample Images}
% Grid of Fresh vs.\ Dry examples before and after preprocessing.

\end{document}
